<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/kowshikchills.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/kowshikchills.github.io/" rel="alternate" type="text/html" /><updated>2020-08-16T01:21:33+05:30</updated><id>http://localhost:4000/kowshikchills.github.io/feed.xml</id><title type="html">KD Explains</title><subtitle>Joint Initiative by Kowshik and Dharani</subtitle><entry><title type="html">KD Explains: A Joint Initiative by Kowshik and Dharani</title><link href="http://localhost:4000/kowshikchills.github.io/KD-Explains-A-Joint-Initiative-by-Kowshik-and-Dharani/" rel="alternate" type="text/html" title="KD Explains: A Joint Initiative by Kowshik and Dharani" /><published>2020-08-07T00:00:00+05:30</published><updated>2020-08-07T00:00:00+05:30</updated><id>http://localhost:4000/kowshikchills.github.io/KD-Explains-A%20Joint%20Initiative%20by%20Kowshik%20and%20Dharani</id><content type="html" xml:base="http://localhost:4000/kowshikchills.github.io/KD-Explains-A-Joint-Initiative-by-Kowshik-and-Dharani/">&lt;p&gt;Being avid readers of blogs and books, we wanted to come up with blogs that gives readers a great perspective with real world applications and get the gist that sticks to mind. Our intention is to untangle intricate subjects to sow enduring knowledge in the readers mind. Not only technical we also bring in various topics like history, new innovations, finance etc., We really respect readers interests and whatever we write is solely our perspective. Let’s all try to improve the knowledge and make the world a better place to learn. Thank You!&lt;/p&gt;

&lt;h3 id=&quot;kowshik-chilamkurthy&quot;&gt;Kowshik Chilamkurthy&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/kowshik_abt.jpg&quot; alt=&quot;centre&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Hailed from IIT Madras. Data science has been my core interest since 5 years with 3 years of industry experience. I currently work in Algorithmic trading. In my experience I understood that with the soaring competition in data science day by day, one has to put an extensive effort to get accustomed with emerging technologies like Reinforcement learning, game theory etc., These interesting and new topics are mostly in the form of research papers or journals. I found there is a need to bring the subject and its applications in a more approchable way. My quench for knowledge includes learning history and other intriguing topics. This inquisitiveness has led me to comprise them all in a lucid way. Hope you learn a new thing from every blog here. Don’t forget to share your feedback 😊&lt;/p&gt;

&lt;h3 id=&quot;dharani-jonnagaladda&quot;&gt;Dharani Jonnagaladda&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/dharani_abt.jpg&quot; alt=&quot;centre&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Graduated from DAIICT, I have joined an analytics firm. I have 2+ experience in data analytics. I believe any data set has a lot of stories to unravel. In my experience when working with real word data sets, not all problems end with deploying a Machine learning model, a complete analysis is expected by any business. So I have sharpend my skills in ML, DL and Visualisation to facilitate the industry need. When I started learning ML and DL, I found existing blogs explain a topic on an over all level and I had to follow some or the other book to get to know the derivations and Math behind it later, which obviously consumed more time. I started to put my learnings in blogs explaining methodically with mathematical equations, derivations and little bit of code that comprehends a topic well. I also keep drawn to learn interesting things happening around the world and if I feel it’s something one needs to know, I keep adding them here. I hope every blog here gives you a good learning point.&lt;/p&gt;</content><author><name>kowshik</name></author><category term="mission" /><summary type="html">Being avid readers of blogs and books, we wanted to come up with blogs that gives readers a great perspective with real world applications and get the gist that sticks to mind. Our intention is to untangle intricate subjects to sow enduring knowledge in the readers mind. Not only technical we also bring in various topics like history, new innovations, finance etc., We really respect readers interests and whatever we write is solely our perspective. Let’s all try to improve the knowledge and make the world a better place to learn. Thank You!</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/kowshikchills.github.io/assets/images/title_pic.jpg" /></entry><entry><title type="html">Reinforcement learning for Covid- 19- Simulation and Optimal Policy</title><link href="http://localhost:4000/kowshikchills.github.io/Reinforcement-learning-for-Covid-19-Simulation-and-Optimal-Policy/" rel="alternate" type="text/html" title="Reinforcement learning for Covid- 19- Simulation and Optimal Policy" /><published>2020-06-24T00:00:00+05:30</published><updated>2020-06-24T00:00:00+05:30</updated><id>http://localhost:4000/kowshikchills.github.io/Reinforcement%20learning%20for%20Covid-%2019:%20Simulation%20and%20Optimal%20Policy</id><content type="html" xml:base="http://localhost:4000/kowshikchills.github.io/Reinforcement-learning-for-Covid-19-Simulation-and-Optimal-Policy/">&lt;p&gt;While the ML community is wondering how they could help the war against the COVID-19 pandemic, I decided to use reinforcement learning to tackle this crisis. This investigation yielded some interesting results in finding the set of optimal actions to fight virus spread.&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h2&gt;

&lt;p&gt;Imagine you are playing a &lt;strong&gt;pandemic control game.&lt;/strong&gt; Your objective is to control the spreading of the virus with the least economic disruption. You can choose between a multitude of actions like ‘close all infected residential areas’, ‘run tests in infected areas’, ‘lockdown’ etc.&lt;/p&gt;

&lt;p&gt;But the immediate question is: how do I quantify economic disruption? Fairly, we can assume that wider the restriction on the movement of the people, the worse the economic health. So our objective is to control the virus spread with the least impediment on the movement of the population.&lt;/p&gt;

&lt;p&gt;What if an algorithm gives you a trained agent that can take actions on your behalf to achieve the goals you set? Would you not employ such an intelligent agent to curb the virus spread? The subject of reinforcement learning(RL) is around modeling such an intelligent agent.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;h1 id=&quot;the-most-exciting-part-of-this-modelling-is-that-we-can-design-an-agent-that-curbs-the-virus-spread-in-the-long-term-with-the-least-disruption-to-the-economic-activity&quot;&gt;The most exciting part of this modelling is that we can design an agent that curbs the virus spread in the long term with the least disruption to the economic activity.&lt;/h1&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;2-reinforcement-learning&quot;&gt;2. Reinforcement Learning&lt;/h2&gt;

&lt;p&gt;Reinforcement Learning is a subfield of machine learning that teaches an agent how to choose an action from its action space. It interacts with an environment, in order to maximize rewards over time. Complex enough? let’s break this definition for better understanding.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Agent&lt;/strong&gt;: The program you train, with the aim of doing a job you specify.&lt;br /&gt; 
&lt;strong&gt;Environment&lt;/strong&gt;: The world in which the agent performs actions.&lt;br /&gt; 
&lt;strong&gt;Action&lt;/strong&gt;: A move made by the agent, which *causes a change *in the environment.&lt;br /&gt; 
&lt;strong&gt;Rewards&lt;/strong&gt;: The evaluation of an action, which is like feedback.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;In any RL modelling task, it’s imperative to define these &lt;strong&gt;4 essential elements&lt;/strong&gt;. Before we define these elements for our Covid-19 problem, let’s first try to understand with an example: &lt;em&gt;how agent learn actions in an environment?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Agent&lt;/strong&gt;: Program controlling the movement of limbs&lt;br /&gt;&lt;br /&gt;
&lt;strong&gt;Environment&lt;/strong&gt;: The real world simulating gravity and laws of motion&lt;br /&gt;&lt;br /&gt;
&lt;strong&gt;Action&lt;/strong&gt;: Move limb L with Θ degrees&lt;br /&gt;&lt;br /&gt;
&lt;strong&gt;Reward&lt;/strong&gt;: Positive when it approaches destination; negative when it falls down.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B1-2.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Agents learn in an interactive environment by &lt;strong&gt;trial and error&lt;/strong&gt; using feedback (Reward) from its own actions and experiences. Agent essentially tries different actions on the environment and learns from the feedback that it gets back. The goal is to find a suitable action policy that would maximize the &lt;strong&gt;total cumulative reward&lt;/strong&gt; of the agent.&lt;/p&gt;

&lt;h2 id=&quot;3-pandemic-control-problem&quot;&gt;3. Pandemic Control Problem&lt;/h2&gt;

&lt;p&gt;Now let’s define these 4 essential elements for our pandemic control problem:
&lt;strong&gt;Agent&lt;/strong&gt;: A Program controlling the movement of the citizens through different actions.&lt;br /&gt; 
&lt;strong&gt;Environment&lt;/strong&gt;: The virtual city where the virus is spreading. By restricting the citizen’s movement, spread dynamics can be altered.&lt;br /&gt; 
&lt;strong&gt;Action&lt;/strong&gt;: Control the movement of the citizens.&lt;br /&gt;&lt;br /&gt;
&lt;strong&gt;Rewards&lt;/strong&gt;: minimise infected from virus spread (pandemic control) +minimise people quarantined( least economic disruption)+ minimise people dead&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Now we need to code-up and discuss each element of this optimal control problem. let’s start with pandemic simulation environment.&lt;/p&gt;

&lt;h2 id=&quot;4-pandemic-simulation-environment&quot;&gt;4. Pandemic Simulation Environment&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;h1 id=&quot;model-the-whole-pandemic-transmission-dynamics-as-interactions-between-different-components&quot;&gt;Model the whole pandemic transmission dynamics as interactions between different components.&lt;/h1&gt;
&lt;/blockquote&gt;

&lt;p&gt;Though there are a large number of pandemic simulation models, I decided to use my own simulation model drawing inspiration from the network model. I choose not to use the standard model because of the following reasons:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;In existing simulation models, the transmission dynamics of the virus does not react to the actions taken by the decision maker/agent. (eg. How would closing public transport impact virus spreading).&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Existing transmission models doesn’t output a comprehensive observation on the state of the city.&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In order to prepare such an environment that overcomes above-mentioned shortcomings, I decided to break the whole pandemic transmission dynamics into interactions between different &lt;strong&gt;components&lt;/strong&gt;.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Let’s discuss these components and their respective assumptions of pandemic simulation environment. We will classify these components into Demographic Components, Transmission Dynamics, Contagious Components.&lt;/p&gt;

&lt;h3 id=&quot;demographic-components&quot;&gt;Demographic Components&lt;/h3&gt;

&lt;p&gt;These are basic components of the simulation model on which the whole transmission dynamics are built. We will create a closed city where we intend to simulate the virus spread. There are assumptions considered about this city, such that the simulation process is less computationally expensive and also close to reality.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B1-3.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;transmission-dynamics&quot;&gt;Transmission Dynamics&lt;/h3&gt;

&lt;p&gt;These transmission dynamics decide the extent and intensity of the virus spread. We can simulate any pandemic using these transmission dynamics.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B1-4.gif&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As you can clearly visualize: Infected citizen makes the daily trip and he/she infects other citizens who came in &lt;strong&gt;contact&lt;/strong&gt; with him with the &lt;strong&gt;probability of transmission&lt;/strong&gt; at each unit. 
We essentially need to define how many citizens come in contact with the infected and what is the probability of transmission at each unit.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B1-5.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;contagious-components-and-simulation-results&quot;&gt;Contagious Components and Simulation Results&lt;/h3&gt;

&lt;p&gt;These contagious components help us build an environment. For a decision maker to take actions to curb the virus spread, he must understand the state of the infected city&lt;em&gt;( eg. number of citizens infected, number of residential areas infected, number of citizens quarantined ,etc).&lt;/em&gt; 
These components facilitate the logging of infected/interaction information in a structured manner. We use the compartment model for simulation. 
Let’s simulate a simple compartment model with infinite hospital capacity. We will randomly infect 3 citizens and simulate a pandemic following the above transmission dynamics.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B1-6.gif&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Contagious Compartment:&lt;/strong&gt; All those active citizens who are infected and contagious are included in this list&lt;br /&gt; 
&lt;strong&gt;Recognized Compartment:&lt;/strong&gt; All those infected who came to the governments notice.&lt;br /&gt;&lt;br /&gt;
&lt;strong&gt;Hospitalized Compartment&lt;/strong&gt;: All those infected citizens recognized by the government will be put in the hospital. Once the infected citizen enter this list, he will be removed from the Contagious Compartment.&lt;br /&gt; 
&lt;strong&gt;Hospital Infrastructure Capacity&lt;/strong&gt;: The capacity of the hospital is limited. Once the capacity reaches, further infected citizens cannot enter the Hospitalized Compartment. This is a very important variable in our simulation, which you will see in plot 6.&lt;br /&gt;&lt;br /&gt;
&lt;strong&gt;Death&lt;/strong&gt;: Infected will be dead as the days progress with the probability proportional to his age&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Let’s look at the simulation results for the pandemic in a city of 1L population and with infinite hospital Infrastructure Capacity and limited(500) capacity. Also, we need to compare it with standard epidemiological models.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B1-7.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B1-8.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;This is a simple epidemiological model. The “ contagious line” in my simulation model(&lt;strong&gt;Plot 6&lt;/strong&gt;) is closer to the “infected line” in SIR model(Plot 7). This clearly implies that the pandemic simulation is accurate.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;5-actions&quot;&gt;5. Actions&lt;/h2&gt;

&lt;p&gt;The need for creating a new environment for the pandemic problem is essentially because we ideally want our pandemic simulation environment to react to the actions taken by the decision maker. So defining action space is as important as defining the environment. 
So by defining wide action space, we are enriching the decision maker’s choices to curb the virus spread.&lt;/p&gt;

&lt;p&gt;The virus spread can be effectively curbed by:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Restricting the movement of the citizens &lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Conducting the tests on probable citizens, so that infected citizens come to the government’s note before the symptoms kick-in.&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;You will now clearly see why I introduced the concept of transmission dynamics. By restricting the movement of the citizens, they are not susceptible to infection anymore. This condition can be easily embedded into the simulation and the dynamics of the virus spread change accordingly.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B1-9.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;These are the actions defined for the decision maker. 
For example, if the decision maker chooses action: 8 (&lt;strong&gt;lockdown&lt;/strong&gt;): then all the citizens in the city cannot move.&lt;/p&gt;

&lt;p&gt;The idea behind defining this action space is that we want to find the most &lt;strong&gt;optimal action policy&lt;/strong&gt; of restricting citizen’s movement. We can design more actions, but for now, we limit to this action space.&lt;/p&gt;

&lt;h2 id=&quot;6-agent-and-rewards&quot;&gt;6. Agent and Rewards&lt;/h2&gt;

&lt;p&gt;Out of 4 essential elements of Reinforcement Learning, we discussed &lt;em&gt;1. Environment 2. Actions&lt;/em&gt; for our pandemic control problem. Let’s discuss agent and reward in this section.&lt;/p&gt;

&lt;p&gt;An &lt;strong&gt;agent&lt;/strong&gt; is essentially a program you train, with the aim of doing a job you specify. &lt;em&gt;But how do we specify the job&lt;/em&gt;? *How can an agent understand your(decision maker) objectives? *The answer is through &lt;strong&gt;reward&lt;/strong&gt;. The agent always try to find out the action policy that maximize the cumulative sum of rewards. So if we can tie the goals of the pandemic control problem with the reward function, we can train an agent which achieves goals for us.&lt;/p&gt;

&lt;p&gt;Let’s reiterate our &lt;strong&gt;objective&lt;/strong&gt;: To control the virus spread with the least impediment on the movement of the population( least economic disruption). 
So we need to minimize:&lt;br /&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Number of people Infected (𝜨𝒊)&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Number of people quarantined(𝜨𝒒)&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Number of people died because of infection(𝜨𝒅)&lt;br /&gt;
We don’t essentially give equal weights to each number. For example, governments don’t let the economy remain healthy at the cost of citizens.&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B1-10.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One thing must be kept in mind when deciding 𝑤𝒊, 𝑤𝒒, 𝑤𝒅. Apart from their ethical importance, these weights are just numbers. We need to choose them judicially such that the agent actually learns to achieve the objectives we set.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;In section 2( RL), we learnt how agent trains. Let’s try to understand the training process in the pandemic control problem. I used the DQN model to train the agent. In this DQN model, the agent tries random actions in the beginning (exploratory) to learn optimal action policy. An interesting concept in this model is &lt;strong&gt;discounted sum of rewards&lt;/strong&gt;: agent gives lesser importance to the immediate rewards and strives to achieve long terms goals.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;I will briefly explain this RL model: Q-learning learns the action-value function &lt;em&gt;Q(s, a)&lt;/em&gt;: &lt;strong&gt;how good to take an action at a particular observation&lt;/strong&gt;.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Let’s try to understand Q value: Consider the pandemic simulation environment, for a given observation:&lt;br /&gt;
&lt;em&gt;{infected, hospitalized, dead, exposed, infected houses, average age of infected}&lt;/em&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Agent will learns Q value &lt;strong&gt;(expected rewards)&lt;/strong&gt; for each action ( Total 16 actions). The agent chooses the action with the highest Q value. We will limit the discussion on RL modelling techniques and jump into the results and Interpretation.&lt;/p&gt;

&lt;h2 id=&quot;7-results-and-interpretation&quot;&gt;7. Results and Interpretation&lt;/h2&gt;

&lt;p&gt;Now we reach the end and also the most interesting part of this blog.&lt;/p&gt;

&lt;p&gt;So let’s create a pandemic simulation in a city of size 1 Lakh. We will let the DQN agent take actions from its action space A &lt;em&gt;(plot 8)&lt;/em&gt; to maximize the reward R&lt;em&gt;( Equation 1).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B1-11.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B1-12.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B1-13.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B1-14.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;8-summary&quot;&gt;8. Summary&lt;/h2&gt;

&lt;p&gt;This modelling and simulation can be extended to cities of different sizes. The actions taken by the agent are more intuitive as the agent understands/learns the pandemic simulation environment better. For example, agents choose to do a lot of tests in infected areas at the beginning of the spread. More action spaces and better reward function makes this whole RL modelling even closer to reality.&lt;/p&gt;

&lt;p&gt;As I mentioned in the beginning, the intention behind writing this blog is to explore the possibility of collaboration and help the war against the corona virus spread. If anyone believes that they can contribute to this RL project, please feel free to mail me kowshikchilamkurthy@gmail.com. Also, I would love to take suggestions from you for better simulation and better RL modelling.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;references:&lt;/em&gt;
1.&lt;a href=&quot;https://en.wikipedia.org/wiki/Compartmental_models_in_epidemiology#The_SIR_model&quot;&gt;https://en.wikipedia.org/wiki/Compartmental_models_in_epidemiology#The_SIR_model&lt;/a&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://blogs.mathworks.com/headlines/2019/05/16/robot-quickly-teaches-itself-to-walk-using-reinforcement-learning/&quot;&gt;https://blogs.mathworks.com/headlines/2019/05/16/robot-quickly-teaches-itself-to-walk-using-reinforcement-learning/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;H. S. Rodrigues, M. T. T. Monteiro, and D. F. M. Torres, “Dynamics of dengue epidemics when using optimal control,” &lt;em&gt;Mathematical and Computer Modelling&lt;/em&gt;, vol. 52, no. 9–10, pp. 1667–1673, 2010.&lt;/li&gt;
&lt;/ol&gt;</content><author><name>kowshik</name></author><category term="ReinforcementLearning" /><category term="tutorial" /><summary type="html">While the ML community is wondering how they could help the war against the COVID-19 pandemic, I decided to use reinforcement learning to tackle this crisis. This investigation yielded some interesting results in finding the set of optimal actions to fight virus spread.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/kowshikchills.github.io/assets/images/B1-1.jpg" /></entry><entry><title type="html">Improve Survival Time in PUBG- A Cox Statistical Approach</title><link href="http://localhost:4000/kowshikchills.github.io/Improve-Survival-Time-in-PUBG-A-Cox-Statistical-Approach/" rel="alternate" type="text/html" title="Improve Survival Time in PUBG- A Cox Statistical Approach" /><published>2020-06-15T00:00:00+05:30</published><updated>2020-06-15T00:00:00+05:30</updated><id>http://localhost:4000/kowshikchills.github.io/Improve%20Survival%20Time%20in%20PUBG:%20A%20Cox%20Statistical%20Approach</id><content type="html" xml:base="http://localhost:4000/kowshikchills.github.io/Improve-Survival-Time-in-PUBG-A-Cox-Statistical-Approach/">&lt;h3 id=&quot;a-real-world-application-of-cox-proportional-hazards-model&quot;&gt;A Real World Application of Cox Proportional-Hazards Model&lt;/h3&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;PUBG needs no introduction. It is one of the popular and the most played games right now. Players fight to death until one remains, so it is a survival game. There are pure statistical models to analyse the survival times. Using **PUBG data, **we will try to use one such survival models to understand how different strategies can improve the player’s survival rates.&lt;/p&gt;

&lt;p&gt;This blog is written for tech, non-tech readers and most importantly PUBG players. I will also include my python implementation for the benefit of tech readers. This can be seen as a sequel to my blog: [&lt;em&gt;The Cox Proportional-Hazards Model](https://medium.com/point-processes/the-cox-proportional-hazards-model-da61616e2e50) and must read for those who are interested in understanding the mathematical background and python implementation of magical *Cox Proportional-Hazards Model.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We will use the data published in Kaggle &lt;a href=&quot;https://www.kaggle.com/skihikingkevin/pubg-match-deaths?select=aggregate&quot;&gt;datasets &lt;/a&gt;where there are over 720,000 PUBG matches. The data log was extracted from &lt;a href=&quot;http://pubg.op.gg/&quot;&gt;pubg.op.gg&lt;/a&gt;, a game tracker website. We will use this data log to understand different modes of game strategies using statistical models and try to figure out the method to evaluate the strategies.&lt;/p&gt;

&lt;h3 id=&quot;a-quick-recap-of-cox-proportional-hazards-model&quot;&gt;A Quick Recap of Cox Proportional-Hazards Model&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;Cox proportional-hazards model&lt;/em&gt; is developed by Cox and published in his work[1] in 1972. It is the most commonly used regression model for survival data. The most interesting aspect of this survival modeling is it’s ability to examine the relationship between survival time and predictors. For example, if we are examining the survival of patients then the predictors can be age, blood pressure, gender, smoking habits, etc. These predictors are usually termed as covariates. &lt;em&gt;Note: It must not be confused with linear regression, the assumptions might be linear in both regression and survival analysis but the underlying concepts are different. Methods we employ for parameter estimations of regression model and survival model are very different from each other.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B7-3.jpg&quot; alt=&quot;centre&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;H&lt;/em&gt;azard function &lt;strong&gt;λ(t)&lt;/strong&gt;: gives the instantaneous risk of demise at time t&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Z: Vector of features/covariates&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;em&gt;λo(t)&lt;/em&gt;&lt;/strong&gt; is called the baseline hazard function&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;pubg-problem-setup--data-engineering&quot;&gt;PUBG Problem Setup &amp;amp; Data Engineering&lt;/h2&gt;

&lt;p&gt;Let’s have a look at the raw data before we define the problem setup.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import pandas as pd
df = pd.read_csv(‘agg_match_stats_0.csv’)
df.head()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B7-4.jpg&quot; alt=&quot;centre&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Feature Description&lt;/em&gt;&lt;strong&gt;: player_size&lt;/strong&gt;: Team Size, &lt;strong&gt;player_dist_ride&lt;/strong&gt;: Distance covered using vehicle by the player , &lt;strong&gt;player_dist_walk&lt;/strong&gt;: Distance walked by the player, &lt;strong&gt;player_kills&lt;/strong&gt;: Number of kills by the player, &lt;strong&gt;players_survive_time&lt;/strong&gt;: time survived by the player&lt;/p&gt;

&lt;h3 id=&quot;problem-setup&quot;&gt;Problem Setup&lt;/h3&gt;

&lt;p&gt;Players in PUBG can choose different strategies to maximise the survival time. We define strategy as a combination of one or more &lt;em&gt;player’s decisions&lt;/em&gt;. Strategies can be something like:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Travel extensively with least confrontation with enemies,&lt;/li&gt;
  &lt;li&gt;Use a motorised vehicle most of the time,&lt;/li&gt;
  &lt;li&gt;Only Walk, but confront with enemies more often, or&lt;/li&gt;
  &lt;li&gt;Even something funnier like: Play only afternoons over the weekends😝.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;There can be 1000’s of such strategies, some of them might look trivial other might not. Our goal is to find a way to evaluate these strategies based on their survival rates. Apart from raw data provided, we also need to engineer these columns to derive meaningful features(player decisions).&lt;/p&gt;

&lt;h3 id=&quot;data-engineering&quot;&gt;Data Engineering&lt;/h3&gt;

&lt;p&gt;In this section, we will briefly discuss the features needed to be extract from the raw data available to use. These features can be simply seen as the decisions taken by the player. Let’s list them and also look at the distributions for some of these features.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B7-5.jpg&quot; alt=&quot;centre&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats
df = pd.read_csv('agg_match_stats_0.csv')
df_features = create_features(df) #func is defined at end of blog
df_features.head()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B7-6.jpg&quot; alt=&quot;centre&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now that we extracted the features, lets jump into the implementation of cox proportional-hazards model.&lt;/p&gt;

&lt;h2 id=&quot;survival-analysis&quot;&gt;Survival Analysis&lt;/h2&gt;

&lt;p&gt;This is the most interesting section: the implementation of cox model in python with the help of lifelines package. It is very important to know about the impact of features on the survival rates. This would help us in predicting the survival rates of a PUBG player, if we know the associated feature values. The Cox model assumes that each features have an impact on the survival rates.&lt;/p&gt;

&lt;p&gt;One of the basic assumptions of the CPH model is that the features are not collinear. We can either solve the issue of multi-collinearity before fitting the cox model or we can apply a penalty to the size of the coefficients during regression. Using a penalty improves stability of the estimates and controls for high correlation between covariates.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from lifelines import CoxPHFitter
cph = CoxPHFitter(penalizer=0.1)
cph.fit(df, duration_col='player_survive_time', event_col='dead')
cph.plot()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B7-7.jpg&quot; alt=&quot;centre&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Coefficients of the features which indicate the measure of the impact on the survival rates of the PUBG player.&lt;/p&gt;

&lt;h3 id=&quot;interpreting-the-summary&quot;&gt;Interpreting the summary&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Hazard ratio (HR) given by exp(coef), where coef is the weight corresposing to the feature. If exp(coef) = 1 for a feature, then it has no effect. If exp(coef) &amp;gt; 1, decreases the hazard: &lt;strong&gt;improves the survival&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;weekend_indi&lt;/em&gt;( that is whether player player over weekend or weekday ) doesn’t play any significant role in predicting his survival risk, whereas &lt;em&gt;player_kills&lt;/em&gt; ( number of kills by player) variable plays significant role in predicting survival risk .&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;game size *feature with exp(coef) = 1.0 has &lt;strong&gt;no effect&lt;/strong&gt; on the survival rates: so it implies that the survival of the player does not depend on the *game size.&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;*%player_dist_ride *feature with exp(coef) = 1.73 (&amp;gt;1) this is good for survival. So preferring the &lt;strong&gt;vehicle&lt;/strong&gt; instead of &lt;strong&gt;walking&lt;/strong&gt; increases the survival rates.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For better understanding of the math behind above deductions, please refer to my earlier blog: &lt;a href=&quot;https://medium.com/point-processes/the-cox-proportional-hazards-model-da61616e2e50&quot;&gt;*The Cox Proportional-Hazards Model&lt;/a&gt;. 
*In the next section, we will also see how different features play together to decide the survival rates of the PUBG player.&lt;/p&gt;

&lt;h2 id=&quot;results--visualisation&quot;&gt;Results &amp;amp; Visualisation&lt;/h2&gt;

&lt;p&gt;The best way we understand impact of each features/decision is that we plot the survival curves for single feature/decision i.e., we keep all other player’s decisions unchanged. we use[&lt;strong&gt;plot_covariate_groups()](https://lifelines.readthedocs.io/en/latest/fitters/regression/CoxPHFitter.html#lifelines.fitters.coxph_fitter.CoxPHFitter.plot_covariate_groups)&lt;/strong&gt; method and give it the feature of interest, and the values to display. Also we will look at the survival rates for different strategies ( combination of decisions)&lt;/p&gt;

&lt;p&gt;In this section we will discuss&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Survival profiles of Decisions&lt;/li&gt;
  &lt;li&gt;Survival profiles of Strategies&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;survival-profiles-of-decisions&quot;&gt;Survival profiles of Decisions&lt;/h3&gt;

&lt;p&gt;One quick way to interpret these different survival curves is that the decision with corresponding survival curve leaning to the right yields more survival probability than that of its left. Let’s try to understand this with an example.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B7-8.jpg&quot; alt=&quot;centre&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B7-9.jpg&quot; alt=&quot;centre&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B7-10.jpg&quot; alt=&quot;centre&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Interpreting plot 3&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;It clearly implies that the survival time of PUBG player increases if he choose to walk instead of taking a vehicle&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;More the distance he traverses, better his survival rates (which is intuitive)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;survival-profiles-of-strategies&quot;&gt;Survival profiles of Strategies&lt;/h3&gt;

&lt;p&gt;Let’s quickly see the survival profile for different strategies. For example, consider these four strategies:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Use vehicles extensively, travel longer distances and kill often&lt;/li&gt;
  &lt;li&gt;Only walk, travel smaller distances and don’t confront with enemies often&lt;/li&gt;
  &lt;li&gt;Do team work, use vehicle less often and travel large distances&lt;/li&gt;
  &lt;li&gt;Select a match with small number of players and kill extensively&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B7-11.jpg&quot; alt=&quot;centre&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The values for decisions are fixed as per the above 4 strategies&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B7-12.jpg&quot; alt=&quot;centre&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Even in the real world survival situations, moving and confronting with the enemies is better than staying idle. We can handcraft 1000’s of such strategies and compare their survival behaviours. We can even understand and approximate the human behaviour during survival situations by applying these kind of statistical model on the data extracted from the survival games.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;We looked at a real world application of Cox proportional-hazards model. We understood how different strategies impact the survival times of the PUBG player. Out of those strategies we analysed, we found strategy of “&lt;em&gt;using vehicles extensively, travelling longer distances and killing often&lt;/em&gt;” statistically promising the longest survival of a PUBG player in a match. There are also neural network variants of Cox proportional-hazards model, we will look at such neural variant of Cox PH model in my next blog in this series.&lt;/p&gt;

&lt;p&gt;Thanks for your time :)&lt;/p&gt;

&lt;p&gt;Here is the full code for reference:&lt;/p&gt;

&lt;p&gt;#import all the dependencies&lt;/p&gt;

&lt;p&gt;import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats&lt;/p&gt;

&lt;p&gt;#import data set
df = pd.read_csv(‘agg_match_stats_0.csv’)&lt;/p&gt;

&lt;p&gt;#Create new features&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def create_features(df)
    df['player_survive_time'] = df['player_survive_time']/60
    df['date'] = [i.split('+')[0] for i in df['date'].values]
    df['date'] = pd.to_datetime(df['date'],format='%Y-%m-%dT%H:%M:%S')
    df['dayofweek_num']=df['date'].dt.dayofweek  
    df['dayofweek_name']=df['date'].dt.weekday_name
    df['Hour'] = df['date'].dt.hour 
    df['weekend_indi'] = 0       
    df.loc[df['dayofweek_num'].isin([5, 6]), 'weekend_indi'] = 1
    df['time_of_day'] = 0       
    df.loc[df['Hour'].isin([24,1,2,3,4,5,6]), 'time_of_day'] = &quot;LateNight&quot;
    df.loc[df['Hour'].isin([7,8,9,10,11]), 'time_of_day'] = &quot;Morn&quot;
    df.loc[df['Hour'].isin([12,13,14,15,16,17,18,19]), 'time_of_day'] = &quot;Evening&quot;
    df.loc[df['Hour'].isin([20,21,22,23]), 'time_of_day'] = &quot;Night&quot;
    df['%player_dist_ride'] = df['player_dist_ride']/(df['player_dist_ride']+df['player_dist_walk'])
    df['%player_dist_walk'] = df['player_dist_walk']/(df['player_dist_ride']+df['player_dist_walk'])
    df['total distance'] = df['player_dist_ride']+df['player_dist_walk']
    df['only_walk'] = 0       
    df.loc[df['%player_dist_walk'].isin([1]), 'only_walk'] = 1
    for i in ['date','team_id','team_placement','player_name','player_dmg','player_dbno','player_dist_ride',
             'player_dist_walk','Hour','dayofweek_num','dayofweek_name','match_id','match_mode']:
        del df[i]
    one_hot = pd.get_dummies(df['time_of_day'])
    df = df.drop('time_of_day',axis = 1)
    df = df.join(one_hot)
    del df[0]
    df['dead'] = 1
    df = df.dropna()
    return(df)


df_sampled = create_features(df)

from lifelines import CoxPHFitter
cph = CoxPHFitter(penalizer=0.1)
cph.fit(df_sampled, duration_col='player_survive_time', event_col='dead')


'''

Individual features 
'''
cph.plot_covariate_groups('only_walk', [0,1], cmap='coolwarm')
plt.xlabel('time (minutes)')
plt.ylabel('Survival Curve')

'''
strategies

'''
df_strategy = pd.DataFrame()
df_strategy['game_size'] = [60,60,60,30]
df_strategy['party_size'] = [2,2,2,1]
df_strategy['player_assists'] = [1,0,4,1]
df_strategy['player_kills'] = [6,1,2,5]
df_strategy['weekend_indi'] = [0,0,0,0,]
df_strategy['%player_dist_ride'] = [0.8,0,0.2,0.5]
df_strategy['%player_dist_walk'] = [0.2,1,0.8,0.5]
df_strategy['total distance'] = [9000,3000,7000,4000]
df_strategy['only_walk'] = [0,1,0,0]
df_strategy['Evening'] = [1,0,1,1]
df_strategy['LateNight'] = [0,1,0,1]
df_strategy['Morn'] = [0,0,1,0]
df_strategy['Night'] = [0,0,0,0]
df_strategy.index = ['Strategy 1','Strategy 2','Strategy 3','Strategy 4']

cph.predict_survival_function(df_strategy).plot()
plt.xlabel('time (minutes)')
plt.ylabel('Survival Curve')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>kowshik</name></author><category term="CoxProcess" /><summary type="html">A Real World Application of Cox Proportional-Hazards Model</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/kowshikchills.github.io/assets/images/B7-2.jpg" /></entry><entry><title type="html">Fundamentals of Reinforcement Learning</title><link href="http://localhost:4000/kowshikchills.github.io/Fundamentals-of-Reinforcement-Learning/" rel="alternate" type="text/html" title="Fundamentals of Reinforcement Learning" /><published>2020-06-14T00:00:00+05:30</published><updated>2020-06-14T00:00:00+05:30</updated><id>http://localhost:4000/kowshikchills.github.io/Fundamentals%20of%20Reinforcement%20Learning</id><content type="html" xml:base="http://localhost:4000/kowshikchills.github.io/Fundamentals-of-Reinforcement-Learning/">&lt;h3 id=&quot;learning-decisions-that-makes-the-difference&quot;&gt;&lt;em&gt;Learning decisions that makes the difference&lt;/em&gt;&lt;/h3&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Designing machine that learn to do a job by itself is one of the most researched topic than any other in recent times due to various reasons like increased computational power, availability of resources to experiment etc., This lead to uncover significant innovations that made life simpler. If you just have data then an algorithm will provide insights or you train a model and it recognizes your face and many other use cases that we see around us which are built using Machine Learning and Deep Learning. Reinforcement Learning is burgeoning by gaining a lot of attention due to its proven capability in making &lt;strong&gt;sequential&lt;/strong&gt; &lt;strong&gt;decision&lt;/strong&gt; process.&lt;/p&gt;

&lt;h2 id=&quot;concepts-of-rl&quot;&gt;Concepts of RL&lt;/h2&gt;

&lt;p&gt;Reinforcement Learning basically consists of an &lt;strong&gt;agent&lt;/strong&gt;(decision maker) that tries to learn from a &lt;strong&gt;state&lt;/strong&gt; in a given surrounding that it interacts called &lt;strong&gt;environment&lt;/strong&gt; and changes its state because of some &lt;strong&gt;action&lt;/strong&gt; taken as per the feedback provided by the environment during the &lt;strong&gt;episode&lt;/strong&gt;. This feedback is numerical(positive, negative or zero) and is called a &lt;strong&gt;reward&lt;/strong&gt;. The optimal behavior of an agent is to learn such that it always gets good feedback i.e., maximize the reward by taking suitable actions. So in RL we are providing the scenario to the agent and it can figure out itself or discover how to take decisions in the most applaudable way.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/RL1-2.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Lets understand the terms used in RL using the very well known PUBG game as a simple example:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The player in PUBG is an &lt;strong&gt;agent&lt;/strong&gt; here and battleground is his &lt;strong&gt;environment&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A complete game played is an &lt;strong&gt;episode&lt;/strong&gt; and walking, running etc are &lt;strong&gt;states&lt;/strong&gt; — helps to pick actions&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The agent has number of &lt;strong&gt;actions&lt;/strong&gt; to take like moving left, right, front and back, run, fire, kill, bend, jump, change gun etc.,&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;strong&gt;reward&lt;/strong&gt; the agent gets here is positive if he kills and zero if he survives with the help of his teammates till the end or negative if he gets shot by other player.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In order to win the game we have to maximize the reward by taking suitable actions at each time frame. Simply we start from a state, take an action and change to another state and get a reward for that action and repeat the process to learn more about the environment setting.&lt;/p&gt;

&lt;p&gt;There are few challenges RL has before us. Some of them are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Trade-offs: As we understood that agent has to optimize the rewards and also it has to interact continuously with the environment i.e., it needs to explore a lot. This leads to a trade off between exploration and exploitation. It has to choose whether it should keep exploring new states which might result in lower reward or take the path that has already seen and got quite a good set of rewards.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Generalization: Can the agent understand or learn if the actions are good/bad in its previously unseen states.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Delayed consequences: We also need to understand if an agent gives a high reward in a current state, it is because of just this state or a series of decisions that it has taken to reach this state?&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are few key concepts that are applicable in RL. A good knowledge on these will let us understand the formulation of agent’s decision process and model of the environment.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/RL1-3.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Markov Property:&lt;/strong&gt; If an agent changes from one state to other it is called a transition and the probability at which it makes the transition is called transition probability. Generally if we have all the probabilities of an agent going from one state to the other, then it is represented in a transition matrix. Markov property says “&lt;em&gt;Future is independent of past given present&lt;/em&gt;”. The equation below depicts the probability of transitioning to next state St+1 in time t is only dependent on the current state St and the action taken At and is independent of the history&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/RL1-4.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is the transition probability matrix which has all the transition probabilities of all states. For example, P12 depicts the probability of transitioning from state 1 to state 2 and so on.&lt;/p&gt;

&lt;p&gt;When we traverse through a set of states in the environment which follows Markov Property, then it is called a Markov chain. They might include random set of states in the Markov chain that also have transition probabilities and we can compute the optimal chain that results in high reward.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In RL, we are more concerned about optimizing the total reward that an agent receives from the environment rather than the immediate reward it gets by transitioning from one state to the other. So we measure the optimality using a function called &lt;strong&gt;return(Gt)&lt;/strong&gt; which is sum of rewards the agent received from time t (Eq 1).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In many games like Atari, alpha go, chess or PUBG we know the game is going to terminate after certain time steps. If this is the setting then it is called an &lt;strong&gt;episodic&lt;/strong&gt; task. If we start another game then we are initiating a new episode so episodes are &lt;strong&gt;independent&lt;/strong&gt; of each other. There can also be problems where it is not going to have an end like certain robots that are used for personal assistance which do not terminate until an external signal from environment puts it in termination state. These are called &lt;strong&gt;continuous&lt;/strong&gt; tasks.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In episodic tasks we can calculate the returns which is a total sum of its rewards till the termination but in continuous tasks as there is no termination, the rewards add up to &lt;strong&gt;infinity&lt;/strong&gt; while calculating the returns. So we introduce a discount factor gamma&lt;strong&gt;(ɤ)&lt;/strong&gt; to calculate the returns in continuous tasks by discounting it. It has it values from 0–1. It plays a crucial role in determining if we give importance to immediate rewards or future rewards. If &lt;strong&gt;ɤ=0&lt;/strong&gt; then the attention lies on immediate rewards and if &lt;strong&gt;ɤ=1&lt;/strong&gt; then on future rewards.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/RL1-5.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If we have a problem statement which says you get a reward of 1 for performing certain action for next k time steps with a discount factor 0.8 and 0.2 then the returns would be&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/RL1-6.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;We see that Gt with &lt;strong&gt;ɤ=0.8&lt;/strong&gt; is yielding a good return even in future but for &lt;strong&gt;ɤ=0.2 **the returns are high only in the immediate time step and in future it is almost tending to zero. So based on the problem statement we can set **ɤ **that facilitates to decide the importance of either **immediate&lt;/strong&gt; or **future **rewards.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We now know an agent changes its states and gets a reward for that transition. Lets check an example to understand in detail:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/RL1-7.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Consider a situation where a student is an agent and he have four states Home, School, Class, Movie and a discount factor of 0.8. The probabilities of transitioning from one state to other is shown in blue boxes and rewards are shown in brown boxes. Agent might have many episodes i.e., a sequence of traversing through states. For example,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Home -&amp;gt; School -&amp;gt; Class -&amp;gt; Home -&amp;gt; Terminate — Lets calculate the returns of state Home. G = 3 + 5&lt;em&gt;0.8 + 5&lt;/em&gt;0.8*0.8 = 10.2&lt;/li&gt;
  &lt;li&gt;Home -&amp;gt; School -&amp;gt; School -&amp;gt; Movie -&amp;gt; Home -&amp;gt; Terminate — Returns in this episode is G = 3 + 2&lt;em&gt;0.8 + (-10)&lt;/em&gt;0.8&lt;em&gt;0.8 + 3&lt;/em&gt;0.8&lt;em&gt;0.8&lt;/em&gt;0.8 = — 0.264&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;It is clear that episode 1 results in high return than episode 2. So it would be feasible to follow it. Returns is a significant concept as it can decide the agents optimal path.&lt;/p&gt;

&lt;h2 id=&quot;markov-reward-processmrp&quot;&gt;Markov Reward Process(MRP):&lt;/h2&gt;

&lt;p&gt;MRP is a Markov process setting which specifies a reward function and a discount factor &lt;strong&gt;ɤ&lt;/strong&gt;. It is formally represented using the tuple (S, P, R, γ) which are listed below:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;S : A finite state space.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;P : A transition probability model that specifies P(s`&lt;/td&gt;
          &lt;td&gt;s).&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;R : A reward function that maps states to rewards (real numbers) R(s) = E[ri&lt;/td&gt;
          &lt;td&gt;si = s] , ∀ i = 0, 1, . . . .(E here is expected value and i is every time step)&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;γ: Discount factor — lies between 0 and 1.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;State Value Function&lt;/strong&gt;: State Value function Vt(s) is the expected sum of returns starting from state s at a time t.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/RL1-8.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Simply put, value function denotes how good it is for an agent to be in that particular state. Transitioning between states that result in a high reward during episodes is an optimal MRP. We have different methods to evaluate V(s). They are&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Monte-Carlo Simulation Method: In MRP, for each episode returns are calculated and are averaged. So State Value function is calculated as Vt(s)=Sum(Gt)/Number of episodes.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Analytic Solution: If the number of time steps are infinite, then we cannot calculate sum or average of returns. In this process, we define γ&amp;lt;1 and State value function is equal to sum of Immediate Reward(reward obtained for transitioning from State s to s’) and discounted sum of future rewards. The equation can be represented in Matrix form as V=R + γPV. Rearranging gives V by multiplying inverse matrix of (I − γP) with R.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/RL1-9.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Iterative Solution: In this method we calculate Value Function at time step t by iterating through its previous value functions at time t-1,t-2 etc., We will look into this in depth soon.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;markov-decision-process-mdp&quot;&gt;Markov Decision Process (MDP):&lt;/h2&gt;

&lt;p&gt;An MDP is simply an MRP but with the specifications of a set of actions that an agent can take from each state. It is represented a tuple (S, A, P, R, γ) which denotes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;S : A finite state space.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A : A finite set of actions which are available from each state s.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;P : A transition probability model that specifies P(s`&lt;/td&gt;
          &lt;td&gt;s).&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;R : A reward function that maps states to rewards (real numbers) R(s) = E[ri&lt;/td&gt;
          &lt;td&gt;Si = s, Ai=a] , ∀ i = 0, 1, . . . .(state s, action taken a, E here is expected value and i is every time step)&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;γ : Discount factor — lies between 0 and 1. An episode of a MDP is thus represented as (s0, a0, r0, s1, a1, r1, s2, a2, r2, . . .).&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;In MRP, we have transition probability of going from one state to the other. In MDP, the notation is slightly changed. We define transition probability with respect to action as well P(Si+1&lt;/td&gt;
      &lt;td&gt;Si , ai). An example of robot transitioning between different states also depends on the action it takes if its moving forward, left, right or halted. All the other notations of returns(Gt), discount factor(γ) are exactly the same as referred in MRP.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;policy-and-q-function&quot;&gt;Policy and Q-Function:&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/RL1-10.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Suppose there is a robot which is currently at a state S1, it can take actions left or right with probabilities al and ar respectively for taking left or right which lands in two different states S2 and S3. Value function and rewards of that state are also mentioned and discount factor = 0.8&lt;/p&gt;

&lt;p&gt;Let us calculate the value function of S1:
V1= al(R+γ&lt;em&gt;V2) + ar(R+γ&lt;/em&gt;V3) 
 = al(2+ 0.8&lt;em&gt;10) + ar(1+ 0.8&lt;/em&gt;15) 
 = al&lt;em&gt;10 + ar&lt;/em&gt;13.5&lt;/p&gt;

&lt;p&gt;If a1 = 0.2 and ar = 0.8, then &lt;strong&gt;V1 = 12.8&lt;/strong&gt; and if a1 = 0.8 and ar = 0.2 then &lt;strong&gt;V1 = 10.7&lt;/strong&gt; Clearly giving more probability to take the action &lt;strong&gt;ar&lt;/strong&gt; will give a better result in terms of value of the state S1.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;To evaluate how good it is to transition to a state is we use &lt;strong&gt;value function&lt;/strong&gt; but to determine how good is it to take an &lt;strong&gt;action&lt;/strong&gt; ‘a’ from this state? This is where the concept of &lt;strong&gt;Policy&lt;/strong&gt; sets in.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;policy-π&quot;&gt;Policy π:&lt;/h3&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Policy is a &lt;strong&gt;decision making&lt;/strong&gt; mechanism in MDP that maps states to actions. For a policy π, if at time t the agent is in state s, it will choose an action a with probability given by π(a&lt;/td&gt;
      &lt;td&gt;s).&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/RL1-11.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Given a policy &lt;strong&gt;π&lt;/strong&gt; how can we evaluate if its good or not? The intuition is same as in MRP, we calculate the expected rewards. We can define as below:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;State Value Function&lt;/strong&gt; (how good is it to transition to a state): Value function at a given state s of an agent, is the expected returns obtained by following a policy &lt;strong&gt;π&lt;/strong&gt; and reaching to a next state, until we reach a terminal state.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/RL1-12.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;state-action-value-function-or-q--function&quot;&gt;State-Action Value Function or Q — Function:&lt;/h3&gt;

&lt;p&gt;The state-action value function for a state s and action a is defined as the expected return starting from the state St = s at time t and taking the action At = a, and then subsequently following the policy π. It is written mathematically as&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/RL1-13.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So this tells us the value of performing an action a in state s following policy π.&lt;/p&gt;

&lt;p&gt;These are just building blocks of MDP in RL. There are lot more concepts like Bell Man Backup Operator, Finding Optimal Value functions and Policies and Dynamic Programming etc., Lets check them out in my next blog.&lt;/p&gt;

&lt;p&gt;Hope this article has pushed your understanding of RL to some level up.&lt;/p&gt;

&lt;p&gt;Thanks for your time !&lt;/p&gt;</content><author><name>dharani</name></author><category term="RL" /><summary type="html">Learning decisions that makes the difference</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/kowshikchills.github.io/assets/images/RL1-2.jpg" /></entry><entry><title type="html">Strategies for Customer Retention: A Cox Survival Model Treatment</title><link href="http://localhost:4000/kowshikchills.github.io/Strategies-for-Customer-Retention-A-Cox-Survival-Model-Treatment/" rel="alternate" type="text/html" title="Strategies for Customer Retention: A Cox Survival Model Treatment" /><published>2020-06-12T00:00:00+05:30</published><updated>2020-06-12T00:00:00+05:30</updated><id>http://localhost:4000/kowshikchills.github.io/%20Strategies%20for%20Customer%20Retention:%20A%20Cox%20Survival%20Model%20Treatment</id><content type="html" xml:base="http://localhost:4000/kowshikchills.github.io/Strategies-for-Customer-Retention-A-Cox-Survival-Model-Treatment/">&lt;h4 id=&quot;techniques-to-devise-personalized-strategies-using-statistical-models&quot;&gt;Techniques to devise personalized strategies using statistical models&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/TP1-2.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Customer churn occurs when customers or subscribers discontinue their association with a company or service. There are many Machine Learning models to predict if a customer is going to churn or not. The problem doesn’t stop there, business has to deploy certain strategies to retain the customers who are at the verge of churn because it’s &lt;a href=&quot;https://www.forbes.com/sites/jiawertz/2018/09/12/dont-spend-5-times-more-attracting-new-customers-nurture-the-existing-ones/#4b86da1f5a8e&quot;&gt;five times cheaper&lt;/a&gt; to retain an existing customer than to acquire a new one. Statistical models can be used to derive and evaluate personalized strategies which is a core challenge in CPG companies.&lt;/p&gt;

&lt;p&gt;We call the event of customer churn as &lt;strong&gt;&lt;em&gt;failure&lt;/em&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;em&gt;survival time&lt;/em&gt;&lt;/strong&gt; is the time taken for such failure/churn. Survival models are statistical techniques used to estimate the time span taken for an event to occur. Cox Proportional-Hazards model is a popular statistical model for survival analysis. Using churn data set from Kaggle, we will try to use this model to understand customer behavior and compare different strategies that can improve customer retention.&lt;/p&gt;

&lt;p&gt;Please refer to &lt;a href=&quot;https://medium.com/point-processes/the-cox-proportional-hazards-model-da61616e2e50&quot;&gt;this blog&lt;/a&gt; to understand Mathematical Equations and reason behind using this.&lt;/p&gt;

&lt;h2 id=&quot;road-map-to-enhance-customer-retention-rate&quot;&gt;Road map to enhance customer retention rate:&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Customer’s characteristics and demographics play a pivotal role in understanding retention behavior. Our goal is to understand the relation between these features and survival time(time taken to churn). We can plot survival/retention curves that are specific to a customer to gain valuable insights.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Devise personalized strategies(for example, increase incentives/offers)for high-valued customers for different survival risk segments during the time. Our goal is to evaluate and compare how they improve the survival/retention behavior in a customer.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;a-quick-recap-of-cox-proportional-hazards-model&quot;&gt;A Quick Recap of Cox Proportional-Hazards Model&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Cox proportional-hazards model&lt;/em&gt; is developed by Cox and published in his work[1] in 1972.The most interesting aspect of this survival modeling is its ability to examine the relationship between survival time and predictors. For example, if we are examining the survival of patients then the predictors can be age, blood pressure, gender, smoking habits, etc. These predictors are usually termed as covariates.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/TP1-3.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;H&lt;/em&gt;azard function &lt;strong&gt;λ(t)&lt;/strong&gt;: gives the instantaneous risk of demise at time t&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Z: Vector of features/covariates&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;em&gt;λo(t)&lt;/em&gt;&lt;/strong&gt; is called the baseline hazard function: Describes how the risk of event changes over time. It is underlying hazard with all covariates equal to 0.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;model-implementation-on-churn-data-set&quot;&gt;Model Implementation on churn data set:&lt;/h2&gt;

&lt;h3 id=&quot;problem-setup--data-engineering&quot;&gt;Problem Setup &amp;amp; Data Engineering&lt;/h3&gt;

&lt;p&gt;I have taken telecom customer churn data set. Lets check the data structure:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import pandas as pd
df = pd.read_csv(‘Data_Churn_Telecom_Cox.csv’)
df.head()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/TP1-4.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/TP1-5.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;These &lt;strong&gt;features&lt;/strong&gt; gives the customer’s &lt;strong&gt;demographics&lt;/strong&gt; and &lt;strong&gt;characteristics&lt;/strong&gt; / &lt;strong&gt;behaviour&lt;/strong&gt;. There are 96 such features.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;“&lt;strong&gt;Total number of months in service&lt;/strong&gt;” column gives us the survival/retention time of a customer. “&lt;strong&gt;churn&lt;/strong&gt;” column gives whether customer churns or not i.e., event occurrence.&lt;/p&gt;

&lt;h3 id=&quot;data-engineering&quot;&gt;Data Engineering&lt;/h3&gt;

&lt;p&gt;Listed down are the &lt;em&gt;feature engineering steps&lt;/em&gt; and we also look at the distributions for some of these features. Code for each feature engineering step is published at end of the blog.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/TP1-6.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A fairly simple assumption is &lt;strong&gt;proportional hazards&lt;/strong&gt;, which is crucial in Cox regression that is included in its name(the Cox proportional hazards model). It means that the &lt;em&gt;ratio&lt;/em&gt; of the hazards for any two individuals is constant over time. We drop those features if they don’t pass this condition.&lt;/p&gt;

&lt;h2 id=&quot;survival-analysis&quot;&gt;Survival Analysis&lt;/h2&gt;

&lt;p&gt;Here comes the most interesting section: the implementation of cox model in python with the help of lifelines package. Understanding the impact of features on survival rates helps us in predicting the retention of a customer profile. The Cox model assumes that each feature have an impact on the survival/retention rate.&lt;/p&gt;

&lt;p&gt;One of the basic assumptions of the CPH model is that the features are not collinear. We can either solve the issue of multi-collinearity before fitting the Cox model or we can apply a penalty to the size of the coefficients during regression. Using a penalty improves stability of the estimates and controls high correlation between covariates.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from lifelines import CoxPHFittr
cph = CoxPHFitter(penalizer=0.1)
cph.fit(df_f, duration_col=’Total number of months in service’, event_col=’churn’)
cph.summary #output2
cph.plot   #output3
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/TP1-7.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;interpreting-the-summary&quot;&gt;Interpreting the summary&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Hazard ratio (HR) given by exp(coef), where coef is the weight corresponding to the feature. If exp(coef) = 1 for a feature, then it has no effect. If exp(coef) &amp;gt; 1, decreases the hazard: &lt;strong&gt;improves the survival&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;number of unique subscribers in the household&lt;/em&gt; has HR = 1.35 which improves survival/retention. &lt;em&gt;mean number of unanswered data calls&lt;/em&gt; has HR = 0.16 implies it has bad effect on survival rate.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results--visualization&quot;&gt;Results &amp;amp; Visualization&lt;/h2&gt;

&lt;p&gt;The best way to understand impact of each features/decision is to plot the survival curves for single feature/decision by keeping all other customers characteristics/demographics unchanged. we use [&lt;strong&gt;plot_covariate_groups()](https://lifelines.readthedocs.io/en/latest/fitters/regression/CoxPHFitter.html#lifelines.fitters.coxph_fitter.CoxPHFitter.plot_covariate_groups)&lt;/strong&gt; method and pass the arguments — feature of interest and the values to display.&lt;/p&gt;

&lt;h3 id=&quot;survival-profiles-of-features&quot;&gt;Survival Profiles of Features&lt;/h3&gt;

&lt;p&gt;One quick way to interpret these different survival curves is that the decision with corresponding survival curve leaning to the right yields more survival/retention probability than that to its left. A plot below for Average monthly revenue over the life of the customer =400 has more survival probability as it is to the right compared to that of 10 which is to its left.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/TP1-8.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;personalized-strategies-for-customers&quot;&gt;Personalized strategies for Customers&lt;/h3&gt;

&lt;p&gt;We can plot the survival profiles for each customer and analyse the reasons for low survival/retention rates by looking at customer features. From above discussions, we already know what actions can improve the survival rates of the customer.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/TP1-9.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can plot the survival profiles of each customer. For time being let’s consider customer with ID 1032424 and compare the two strategies as shown below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/TP1-10.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can clearly see that strategy 1( i&lt;em&gt;ssue more models to the customer : 1032424&lt;/em&gt;) has comparatively longer survival time than strategy 2(Reduce revenue generated from the customer by giving offer). Similarly, we can analyse each and every customer and design proactive strategies to ensure highest retention statistically. We can also compare different strategies developed by business intelligence teams and deploy based on the effectiveness of a strategy to retain customers.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;We segmented customer behavior by grouping them based on average monthly revenue brackets, number of models issued etc., and Cox proportional-hazards model enabled us to derive personalized strategies to reduce the churn rate. Not only deriving personalized strategies, we learnt to compare them. As an example, for one customer we saw in above section, statistically proves that issuing more models will have a better impact than providing incentives/offers for a customer to stick to the company for a longer time span. This is an outstanding way to meet the landscape of customer expectations and increase customer engagement with the company.&lt;/p&gt;

&lt;p&gt;Thanks for your time :)&lt;/p&gt;

&lt;p&gt;Full code for reference:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import pandas as pd
df = pd.read_csv('Data_Churn_Telecom_Cox.csv')
with open('cols.txt') as f:
   lines = f.readlines()
cols_names = {}
for i in range(len(lines)):
   for j in df.columns:
       if j == lines[i][:-1]:
           cols_names[j] = lines[i+1][:-1]
cols_names['Customer_ID'] = 'Customer_ID'
df.columns =   cols_names.values()


df['churn'] = df['Instance of churn between 31-60 days after observation date']
del  df['Instance of churn between 31-60 days after observation date']
df = df.dropna()

del df['N']
df.set_index('Customer_ID', inplace=True)
'''
Drop categorical features with unique values &amp;gt;2
'''
import numpy as np
df_str = df.loc[:, df.dtypes == object]
for i in df_str.columns:
   if len(np.unique(df_str[i].values)) &amp;gt;2:
       del df[i]
'''
One hot encoding
'''
df_str = df.loc[:, df.dtypes == object]
for i in df_str.columns:
   one_hot = pd.get_dummies(df[i])
   one_hot.columns = [i +'_'+j for j in one_hot.columns]
   df = df.drop(i,axis = 1)
   df = df.join(one_hot)

survival_time = df['Total number of months in service'].values
del df['Total number of months in service']
churn = df['churn'].values
del df['churn']


'''
Drop correlated features
'''

corr_matrix = df.corr().abs()
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))
to_drop = [column for column in upper.columns if any(upper[column] &amp;gt; 0.98)]
df.drop(to_drop, axis=1, inplace=True)



df = df[list(df.columns[:69])+['Credit card indicator_N']]
df['Total number of months in service'] = survival_time
df['churn'] = churn
df = df[df['churn'] == 1]

'''
Select valuable features
'''
df_sampled = df.sample(n=1000)
from lifelines import CoxPHFitter
cph = CoxPHFitter(penalizer=0.01)
cph.fit(df_sampled, duration_col='Total number of months in service', event_col='churn')
df_stats = cph.summary
features_valuable = list(df_stats[df_stats['exp(coef)'].values &amp;gt; 1.01].index) + list(df_stats[df_stats['exp(coef)'].values &amp;lt; 0.98].index)
df = df[features_valuable+['churn','Total number of months in service']]



from lifelines import CoxPHFitter
cph = CoxPHFitter(penalizer=0.01)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>dharani</name></author><category term="CoxProcess" /><summary type="html">Techniques to devise personalized strategies using statistical models</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/kowshikchills.github.io/assets/images/TP1-1.jpg" /></entry><entry><title type="html">Game Theory- The Genius of Nash</title><link href="http://localhost:4000/kowshikchills.github.io/Game-Theory-The-Genius-of-Nash/" rel="alternate" type="text/html" title="Game Theory- The Genius of Nash" /><published>2020-06-10T00:00:00+05:30</published><updated>2020-06-10T00:00:00+05:30</updated><id>http://localhost:4000/kowshikchills.github.io/Game%20Theory:%20The%20Genius%20of%20Nash</id><content type="html" xml:base="http://localhost:4000/kowshikchills.github.io/Game-Theory-The-Genius-of-Nash/">&lt;p&gt;We discussed strict dominance solution concept in great detail in the last blog. Its application is limited and only applicable to some section of games( Games with strict dominant strategy). Strict dominant strategy often fails to exist. 
Lets consider &lt;strong&gt;Battle of sexes&lt;/strong&gt; game.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B2-2.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Dominant strategy equilibrium did not apply, because there is no dominant strategy. 
In the last blog, we discussed the concept of belief. Player will behave optimally( best response ) to their beliefs. Chris may behave optimally and go to football given his belief that Alex is going to the football game. But their beliefs can be wrong.&lt;/p&gt;

&lt;p&gt;In this blog, we will discuss one of the most central and best known solution concept in the game theory. This overcomes many shortcoming faced by other solution concepts, this is developed by &lt;strong&gt;John Nash&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Let’s define Nash’s solution concept. Nash equilibrium is as a profile of strategies(defined in the last blog) for which each player is choosing a best response to the strategies of all other players.
Each strategy in a Nash equilibrium is a best response to all other strategies in that equilibrium
 Lets formally define nash equilibrium&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt;: The pure-strategy profile &lt;strong&gt;s&lt;em&gt;= (s&lt;/em&gt;&lt;/strong&gt;₁&lt;strong&gt;, s&lt;/strong&gt;*₂&lt;strong&gt;, . . . , s&lt;em&gt;n) ∈ S** is a Nash equilibrium if **s&lt;/em&gt;ᵢ&lt;/strong&gt; is a best response to &lt;strong&gt;s*₋ᵢ&lt;/strong&gt; , for all i ∈ N, that is,&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;v&lt;/strong&gt;ᵢ&lt;strong&gt;(s∗ᵢ , s∗₋ᵢ) ≥ v&lt;/strong&gt;ᵢ&lt;strong&gt;(sᵢ, s∗₋ᵢ)&lt;/strong&gt; &lt;strong&gt;for all sᵢ ∈ Sᵢ and all i ∈ N.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Please note that s* is strategy profile, not strategy. strategy profile refers of set of actions taken by all the players in a strategic environment/game.
lets try to understand this definition by working out an example.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B2-3.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Consider this matrix representation. Now lets write down all possible strategy profiles. 
&lt;strong&gt;S&lt;/strong&gt; = {(L,U), (C,U),(R,U),(L,M), (C,M),(R,M),(L,D), (C,D),(R,D)}.
Now lets evaluate payoff functions vis-a-vis best response. 
if player 1 chooses U best response for player 2 is L: BR₂(U) = L
&lt;strong&gt;BR&lt;/strong&gt;₂&lt;strong&gt;(U) = L&lt;/strong&gt;, BR₂(M) = C, BR₂(D) = R
&lt;strong&gt;BR&lt;/strong&gt;₁&lt;strong&gt;(L) = U&lt;/strong&gt;, BR₁(C) = D, BR₁(R) = U
Now closely observe If player 2 chooses L, then player 1’s best response is {U}; at the same time, if player 1 chooses U, then player 2’s best response {L}. It clearly fits the definition above. 
So this is the &lt;strong&gt;s*: {L, U}&lt;/strong&gt; Nash equilibrium. 
let’s apply Nash’s solution concept to prisoners dilemma.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B2-4.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;S = {(RS,BE), (BE,BE), (BE,RS), (RS, RS)} 
Nash equilibrium s* is (BE,BE)
I encourage readers to solve this and find out how (BE,BE) is Nash Equilibrium.
Here are the assumptions for a Nash equilibrium:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Each player is playing a best response to his beliefs.&lt;/li&gt;
  &lt;li&gt;The beliefs of the players about their opponents are correct.
We will not dig too deep into these assumptions as it can put us in mid of some philosophical discussion. 
Lets compare Nash solution concept with other solution concepts&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B2-5.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here it easy to deduce that there is strictly dominant strategy for both players: thus strict dominance concept fails.
There is no strictly dominated strategy for any player, so iterative elimination method is not applicable.&lt;/p&gt;

&lt;p&gt;Lets check if a pure-strategy Nash equilibrium does exist.
BR₁(L) = D, &lt;strong&gt;BR&lt;/strong&gt;₁&lt;strong&gt;(C) = M,&lt;/strong&gt; BR₁(R) = M
BR₂(U) = L, &lt;strong&gt;BR&lt;/strong&gt;₂&lt;strong&gt;(M) = C&lt;/strong&gt;, BR₂(D) = L
we find that &lt;strong&gt;(M, C)&lt;/strong&gt; is the &lt;strong&gt;pure-strategy Nash equilibrium&lt;/strong&gt;— and it is unique.&lt;/p&gt;

&lt;p&gt;Solution concept is finest if it predicts or prescribes an unique strategy. It is necessary to understand if Nash equilibrium always yields unique strategy.
Lets consider the battle of sexes game.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B2-6.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let’s solve Nash equilibrium for this game. 
S = {(O, F), (O, O), (F, F), (F,O )}
BRa(O) = O, BRa(F) = F
BRc(O) = O, BRa(F) = F&lt;/p&gt;

&lt;p&gt;We can clearly observe that we may not have a unique Nash equilibrium, but it usually lead to more refined predictions than those of strict dominant solution concept and iterative elimination. 
Nash equilibrium solution concept has been applied widely in economics, political science, legal studies, and even biology.
Let’s discuss an example where we can apply Nash’s solution concept to real life problem.&lt;/p&gt;

&lt;h2 id=&quot;stag-hunt&quot;&gt;Stag Hunt&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;Two individuals go out on a hunt. Each can individually choose to hunt a stag or hunt a hare. Each player must choose an action without knowing the choice of the other. If an individual hunts a stag, they must have the cooperation of their partner in order to succeed. An individual can get a hare by himself, but a hare is worth less than a stag. This has been taken to be a useful analogy for social cooperation, such as international agreements on climate change.The payoff matrix is as follows&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B2-7.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;BR₁(S) = S, BR₁(H) = H
BR₂(H) = H, BR₂(S) = S
Game has two pure-strategy equilibria: (S, S) and (H, H). However, the payoff from (S, S) &lt;strong&gt;Pareto dominates&lt;/strong&gt; that from (H, H).&lt;/p&gt;

&lt;p&gt;If a player anticipates that the other individual is not cooperative, then he would choose to hunt a hare. But if he believes that other individual will cooperate then we would choose stag. When both individuals choose stag i.e when both believe other individual will cooperate, as a whole both of them would be better off.&lt;/p&gt;

&lt;h2 id=&quot;scarce-resource&quot;&gt;Scarce Resource&lt;/h2&gt;

&lt;p&gt;Let’s try to understand how self interested players might behave in scenario of scarce resources. Imaging there are &lt;strong&gt;n&lt;/strong&gt; fertiliser manufacturing companies each choosing how much to produce around a fresh water lake. Each manufacturing companies degrades some amount of fresh water in that lake and uses, Lets say the total units of water in lake is K. Each player i chooses his own consumption of clean water for production, k&lt;strong&gt;ᵢ&lt;/strong&gt; ≥ 0, and the amount of clean water left is therefore &lt;strong&gt;K -⅀ki&lt;/strong&gt; .
The benefit of consuming an amount k&lt;strong&gt;ᵢ&lt;/strong&gt; ≥ 0 gives player i a benefit equal to &lt;strong&gt;ln(kᵢ)&lt;/strong&gt; to the fertiliser company, and no other player benefits from i’s choice.
Each player also enjoys consuming the remainder of the clean air, giving each a benefit ln(K −&lt;strong&gt;⅀&lt;/strong&gt; kj). Hence the total payoff of player i is&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B2-8.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For player i from the choice k= (k₁, k₂, . . . , kn).
To compute Nash equilibrium, we need to find a strategy profile for which all players choose best-response to their beliefs about his opponent). 
That is we find strategy profile (k∗₁, k∗₂, . . . , k∗n) for which k∗&lt;strong&gt;ᵢ&lt;/strong&gt;= BRi(k∗&lt;strong&gt;₋ᵢ&lt;/strong&gt;) for all i ∈ N. For player I, we can get best response the by maximising the value function written above. To find ki, which maximises the value function of industry i, We can equate its derivative to zero.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B2-9.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B2-10.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Solving above equation gives player’s i best response.&lt;/p&gt;

&lt;p&gt;Lets take only 2 industries case and solve this. ki(kj ) be the best response of player i.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B2-11.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Lets plot this with k1 payoff in x axis and k2 payoff in y axis.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B2-12.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If we solve the two best-response functions simultaneously, we find the unique Nash equilibrium, which has both players playing k₁= k₂ = K/3.&lt;/p&gt;

&lt;h2 id=&quot;mixed-strategies&quot;&gt;&lt;strong&gt;Mixed Strategies&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;So far we discussed pure strategies, but we need to discuss the problem where player may choose to randomise between several of his pure strategies. There are many interesting applications to this kind of behaviour where player chooses actions stochastically( i.e. Instead of chooses a single strategy, player chooses a distribution of strategies). The probability of choosing any of pure strategy is nonnegative, and the sum of the probabilities of choosing any all pure strategies events must add up to one.
We will also closely observe applicability of Nash equilibrium to these mixed strategies. In fact, Nash equilibrium is applied to the games only if player chooses mixed strategies instead of pure strategies.&lt;/p&gt;

&lt;p&gt;We start with the basic definition of random play when players have finite strategy sets &lt;strong&gt;Sᵢ&lt;/strong&gt;:
Let &lt;strong&gt;Sᵢ = {sᵢ₁, sᵢ₁, . . . , sᵢm}&lt;/strong&gt; be player i’s finite set of pure strategies. Define &lt;strong&gt;ΔSᵢ **as the simplex of **Sᵢ&lt;/strong&gt;, which is the set of all probability distributions over &lt;strong&gt;Sᵢ&lt;/strong&gt; . A mixed strategy for player i is an element &lt;strong&gt;σᵢ ∈ Sᵢ&lt;/strong&gt;, so that
&lt;strong&gt;σᵢ= {σ(sᵢ₁), σᵢ(sᵢ₂), . . . , σᵢ(sᵢm))&lt;/strong&gt; is a probability distribution over &lt;strong&gt;Sᵢ **,
where **σᵢ(sᵢ)&lt;/strong&gt; is the probability that player i plays s&lt;strong&gt;ᵢ&lt;/strong&gt; .&lt;/p&gt;

&lt;p&gt;Now consider the example of the rock-paper-scissors game, in which S&lt;strong&gt;ᵢ&lt;/strong&gt;= {R, P, S} (for rock, paper, and scissors, respectively). We can define the simplex as 
ΔSi ={(σ&lt;strong&gt;ᵢ&lt;/strong&gt;(R), σ&lt;strong&gt;ᵢ&lt;/strong&gt;(P ), σ&lt;strong&gt;ᵢ&lt;/strong&gt;(S)) : σ&lt;strong&gt;ᵢ&lt;/strong&gt;(R), σ&lt;strong&gt;ᵢ&lt;/strong&gt;(P ), σ&lt;strong&gt;ᵢ&lt;/strong&gt;(S)≥0, σ&lt;strong&gt;ᵢ&lt;/strong&gt;(R)+σ&lt;strong&gt;ᵢ&lt;/strong&gt;(P )+σ&lt;strong&gt;ᵢ&lt;/strong&gt;(S)=1},&lt;/p&gt;

&lt;p&gt;The player i and his opponents -i both choose mixed actions. It implies that player’s i belief about his opponents -i is not fixed but random. Thus a belief for player i is a probability distribution over the strategies of his opponents.&lt;/p&gt;

&lt;p&gt;Definition: A &lt;strong&gt;belief&lt;/strong&gt; for player i is given by a probability distribution &lt;strong&gt;πᵢ∈S₋ᵢ&lt;/strong&gt; over the strategies of his opponents. We denote by &lt;strong&gt;πᵢ(s₋ᵢ)&lt;/strong&gt; the probability player i assigns to his opponents playing &lt;strong&gt;s₋ᵢ ∈ S₋ᵢ&lt;/strong&gt; .
For example in the rock-paper-scissors game, Belief of player i is represented as (&lt;strong&gt;πᵢ&lt;/strong&gt;(R), &lt;strong&gt;πᵢ&lt;/strong&gt;(P ), &lt;strong&gt;πᵢ&lt;/strong&gt;(S)). We can think of σ&lt;strong&gt;*₋ᵢ&lt;/strong&gt; as the belief of player i about his opponents, π&lt;strong&gt;ᵢ&lt;/strong&gt;, which captures the idea that player i is uncertain of his opponents.&lt;/p&gt;

&lt;p&gt;behavior.&lt;/p&gt;

&lt;p&gt;In pure strategy, the payoff is straight forward. In mixed strategy, to evaluate payoff we need to reintroduce the concept of &lt;strong&gt;expected payoff.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B2-13.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The expected payoff of player i when he chooses pure strategy &lt;strong&gt;sᵢ∈ Sᵢ&lt;/strong&gt; and his opponents choose mixed strategy &lt;strong&gt;σ₋ᵢ∈ ΔS−ᵢ&lt;/strong&gt;
Please note that pure strategy is part of mixed strategy.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B2-14.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When player i choose mixed strategy &lt;strong&gt;σᵢ∈ ΔS&lt;/strong&gt;ᵢ and his opponents choose mixed strategy &lt;strong&gt;σ₋ᵢ∈ ΔS₋ᵢ.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B2-15.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let calculate payoff in mixed strategy scenario.&lt;/p&gt;

&lt;p&gt;lets assume that player 2 plays σ₂(R) = 0.5
σ₂(P ) = 0.5
σ₂(S) = 0 
We can now calculate the expected payoff for player 1 if he chooses pure strategy.
V₁(R, σ₂) = 0.5&lt;em&gt;(0)+ 0.5&lt;/em&gt;(-1) + 0 &lt;em&gt;(1)=-0.5
V₁(P, σ₂) = 0.5&lt;/em&gt;(1)+ 0.5&lt;em&gt;(0) + 0 *(-1)= 0.5
V₁(S, σ₂) = 0.5&lt;/em&gt;(-1)+ 0.5*(1) + 0 *(0)=0&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;V₁(P, σ2)&amp;gt;V₁(S, σ2)&amp;gt;V₁(R, σ₂)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To given player 2’s mixed strategy, we see a best response to player 1, which is action P.&lt;/p&gt;

&lt;p&gt;Now let’s understand how Nash equilibrium solution concept applies to mixed strategies. It actually simpler than it looks, we just replace strategy profile with mixed strategy profile.
&lt;strong&gt;Definition:&lt;/strong&gt; The mixed-strategy profile &lt;strong&gt;σ* = (σ&lt;em&gt;₁ , σ&lt;/em&gt;₂ , . . . , σ&lt;em&gt;n )** is a Nash equilibrium if for each player **σ&lt;/em&gt;ᵢ **is a best response to **σ&lt;em&gt;₋ᵢ**. That is, for all i ∈ N, **vᵢ(σ&lt;/em&gt;ᵢ , σ&lt;em&gt;₋ᵢ) ≥ vᵢ(σᵢ, σ&lt;/em&gt;₋ᵢ). ∀ σᵢ∈ Sᵢ&lt;/strong&gt;.
Each mixed strategy in a Nash equilibrium is a best response to all other mixed strategies in that equilibrium.&lt;/p&gt;

&lt;p&gt;Let’s close the discussion on mixed strategies here. We will discuss more about them in the next blog in my blog series. 
Hope you enjoy reading this blog.
Thanks :)&lt;/p&gt;</content><author><name>kowshik</name></author><category term="Game" /><category term="Theory" /><summary type="html">We discussed strict dominance solution concept in great detail in the last blog. Its application is limited and only applicable to some section of games( Games with strict dominant strategy). Strict dominant strategy often fails to exist. Lets consider Battle of sexes game.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/kowshikchills.github.io/assets/images/B2-1.jpg" /></entry><entry><title type="html">Bio Mimicry — Nature Inspired Innovations</title><link href="http://localhost:4000/kowshikchills.github.io/Bio-Mimicry-Nature-Inspired-Innovations/" rel="alternate" type="text/html" title="Bio Mimicry — Nature Inspired Innovations" /><published>2020-06-01T00:00:00+05:30</published><updated>2020-06-01T00:00:00+05:30</updated><id>http://localhost:4000/kowshikchills.github.io/Bio%20Mimicry%20%E2%80%94%20Nature%20Inspired%20Innovations</id><content type="html" xml:base="http://localhost:4000/kowshikchills.github.io/Bio-Mimicry-Nature-Inspired-Innovations/">&lt;p&gt;Nature is so altruistic that we depend on it for every minute thing in life, yet it never ceases to influence us along the timeline. Early man had seen fire erupting due to friction and he rubbed stones to create fire. Every organism in nature has its uniqueness and its ability to change its capabilities according to the situations is a striking phenomenon where we can derive motivation from. A very popular example — Wright Brothers have closely observed pigeon and bats while flying which flashed the idea to design aircraft with wings.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/BM1-2.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Deriving inspiration from nature for invention to a complex human problem is called BioMimicry. The term ‘biomimetics’ was coined in 1950s by American biophysicist and polymath Otto Schmitt. He has worked on neural system in squid and certain findings of his work led him to the invention of an amplifier. Later, Biomimicry was popularized by scientist and author Janine Benyus in her 1997 book &lt;em&gt;Biomimicry: Innovation Inspired by Nature&lt;/em&gt;. She defines biomimicry as “new science that studies nature’s models and then imitates or takes inspiration from these designs and processes to solve human problems”.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If you have seen the leaves of Lotus, the adhesiveness of the leaf with water particles is really low. This ultrahydrophobicity which means the leaf does not allow water to stay on it due to its versatile architecture of having wax crystals like small buds that trap air. When a droplet falls on it, the arrangement in its cells allow it to have minimal contact area with the droplet and repel the drops instead of making it wet. This idea has led to the invention of &lt;strong&gt;dust proof paints&lt;/strong&gt; and &lt;strong&gt;water proof cloths&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/BM1-3.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Termites are not widely known for their excellence in creating impressive &lt;strong&gt;ventilation&lt;/strong&gt; systems for cooling. They build intricate connections to pass on air flowing rather than stuck at a place and cause hot suffocated air in the room. This has stirred the idea in many architects to have elaborate ventilation system to keep buildings cool &lt;strong&gt;minimizing&lt;/strong&gt; &lt;strong&gt;electric&lt;/strong&gt; &lt;strong&gt;energy&lt;/strong&gt; consumption.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/BM1-4.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Having transparent glasses in tall buildings is a beauty of architectural fineness but birds flying at that height cannot really see the glass as its completely transparent and might die by hitting them. Architects have found a way by observing spider web which shines in light that can draw attention of birds. They have designed special shiny glasses that can alert birds but they look like a normal transparent glass to us. They are known as &lt;strong&gt;spider glasses&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/BM1-5.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If you have ever observed the pace at which &lt;strong&gt;wood pecker&lt;/strong&gt; drills a tree then you might afraid with the pace it drums the surface of a tree with its nose. It can beat 22 times per second with a deceleration of 1200g and a tolerating deceleration of humans is less than 80g. Scientists then observed via CT scans and found that Wood pecker has nearly 4 muscle structures around its neck so as to bear with the mechanical shock while pecking. This has sown the seed of an amazing invention of &lt;strong&gt;shock absorbers&lt;/strong&gt; in automobiles.&lt;/p&gt;

&lt;p&gt;To protect from any injuries while pecking at fast pace where there is a high chance of injuring its head, but its skull is wrapped with a soft tissue of bones which will reduce the effect of hitting hard. This observation was inspired to design &lt;strong&gt;helmets&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/BM1-6.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Sun light is a &lt;strong&gt;renewable&lt;/strong&gt; source of energy which made solar electricity a sustainable source of energy. Many of us might have seen solar panel arranged using a lot of space and they are faced at the direction of sun rise mostly. We know that Sun flower moves according to the direction of Sun light and its petals are arranged in such a way that they are separated and not overlapped to receive enough sun light. This has inspired a new design of solar panel which takes &lt;strong&gt;minimum&lt;/strong&gt; &lt;strong&gt;space&lt;/strong&gt; setup and with the panel that changes its &lt;strong&gt;direction&lt;/strong&gt; as the sun moves along its path.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/BM1-7.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Japan has designed worlds first &lt;strong&gt;bullet&lt;/strong&gt; speed train. They were a huge success in many other parts of the world too. It had a problem while it passes through tunnels in the mountain ranges which are so common in Japan. Due to its speed while passing through tunnel, it made &lt;strong&gt;deafening&lt;/strong&gt; &lt;strong&gt;noise&lt;/strong&gt; that caused trouble to people living near by. Scientists have researched a lot on this and one cannot believe that the bird &lt;strong&gt;kingfisher&lt;/strong&gt; has provided a solution to this!! Yes that’s right! Kingfisher has its nose narrow and pointed. It just dives into water to catch fish with very little splash and noise. One of the engineer who is a bird watcher observed this and wondered if this redesign to face of the train that looks similar to kingfishers beak would reduce the loud boom. Luckily with positive results, the &lt;strong&gt;noise&lt;/strong&gt; &lt;strong&gt;reduced&lt;/strong&gt; and also brought down energy consumption to 15%.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/BM1-8.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Prey trapping mechanism in pitcher plant has provided a solution to design s net that traps insects. The capability of LED lights is maximized by understanding Fireflies mechanism. Qualcomm’s Mirasol display has microscopic reflective units that drew inspiration from colorful butterflies. These screen produce light using reflection and not by producing in the screen itself. These screens has reduced the usage of energy.&lt;/p&gt;

&lt;p&gt;Not only in technology, there are numerous examples where we derive insights from nature to treat diseases. We know there is no vaccine yet for AIDS. An extensive research is going on to increase immunity power in humans by observing alligators that are present in bay area of Louisiana which have extraordinary immune system that can fight hazardous diseases like HIV AIDS. Researching on mosquitoes to design thin needles for extracting blood for diagnosis and operations is another interesting development going on.&lt;/p&gt;

&lt;p&gt;There are several instances where Biomimicry is shaping innovative solutions to solve real world problems. This field is soaring high in developments to provide Millions of jobs in next 10 years with Billions of revenue. There is an institute founded by Janine Benyus to welcome researches to expand this field that has a great potential in giving rise to significant innovations. After all, we all need an easy life :)&lt;/p&gt;

&lt;p&gt;Hope you learnt a new thing today! Thanks for your time!!&lt;/p&gt;</content><author><name>dharani</name></author><category term="Nature" /><summary type="html">Nature is so altruistic that we depend on it for every minute thing in life, yet it never ceases to influence us along the timeline. Early man had seen fire erupting due to friction and he rubbed stones to create fire. Every organism in nature has its uniqueness and its ability to change its capabilities according to the situations is a striking phenomenon where we can derive motivation from. A very popular example — Wright Brothers have closely observed pigeon and bats while flying which flashed the idea to design aircraft with wings.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/kowshikchills.github.io/assets/images/BM1-1.jpg" /></entry><entry><title type="html">Understanding Point Processes</title><link href="http://localhost:4000/kowshikchills.github.io/Understanding-Point-Processes/" rel="alternate" type="text/html" title="Understanding Point Processes" /><published>2020-05-22T00:00:00+05:30</published><updated>2020-05-22T00:00:00+05:30</updated><id>http://localhost:4000/kowshikchills.github.io/Understanding%20Point%20Processes</id><content type="html" xml:base="http://localhost:4000/kowshikchills.github.io/Understanding-Point-Processes/">&lt;h2 id=&quot;understanding-point-processes&quot;&gt;Understanding Point Processes&lt;/h2&gt;

&lt;p&gt;In this world, many events occur and their trends are likely to follow a pattern. In this blog, we try to lay foundations to model those patterns. For example, the likeliness of a new earthquake typically increases in the region where an earthquake has already occurred. This increase in likeliness can be mainly because of the aftershocks created by the earlier earthquake. A panic selling of stocks in one country can cause a similar event in a different country.&lt;/p&gt;

&lt;p&gt;Take an example of wildfire, a wildfire in an amazon forest this year can greatly decrease the occurrence of another wildfire in the coming year. This decrease in the likeliness of wildfire next year is mainly because of the combustion of existing forest fuel. So it is clear that probabilities of a similar event can be elevated or decreased — by patterns in the sequence of previous events.&lt;/p&gt;

&lt;p&gt;If the probabilities of a similar event are elevated i.e., each occurrence increases the rate of future occurrence, like in earthquakes example, then these events can be categorised as &lt;strong&gt;stochastically excited or self- excited&lt;/strong&gt;. If the probability of a similar event is decreased, like in earthquakes example then these events can be categorised as &lt;strong&gt;stochastically inhibited or self-regulating&lt;/strong&gt;. If the probability of a similar event is unaffected, each occurrence doesn’t have any impact on the rate of future occurrence, then these events can modelled as a &lt;strong&gt;Poisson point process&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;theory-of-point-process&quot;&gt;Theory of Point Process&lt;/h2&gt;

&lt;p&gt;A point process is a stochastic model underlying the occurrence of events in time and/or space. In this blog, we will emphasis on purely temporal aspects of point process i.e., the space in which the points fall is simply a portion of the real line which represents time.&lt;/p&gt;

&lt;h3 id=&quot;counting-process-nt-&quot;&gt;Counting Process( &lt;strong&gt;N(t) )&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;To start, consider a line that represents time and event times T₁,T₂,… of event times falling along the line, T&lt;strong&gt;ᵢ&lt;/strong&gt; (event times ) can usually be interpreted as the time of occurrence of the i-th event. This event can be earthquake in a particular region or wildfire in amazon forest. Our job is to model these event times. Instead of modelling these event times T₁,T₂ ,… Tn, it can alternatively be described by a counting process &lt;strong&gt;N(t).&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A counting process &lt;strong&gt;N(t)&lt;/strong&gt; can be viewed as a cumulative count of the number of ‘arrivals’ into a system up to the current time t. If 146 earthquakes had occurred in Himalayans for last 80 years since the seismograph in installed, the N(80) = 146. Simple enough right!
let’s also define history: H(u) history of the arrivals up to time u.&lt;/p&gt;

&lt;h3 id=&quot;conditional-intensity-function--λt-&quot;&gt;Conditional Intensity Function ( λ٭(t) )&lt;/h3&gt;

&lt;p&gt;When we are discussing the concepts of stochasticity, it is pertinent to define a function that gives the expectation of event occurrence at time t. That function is called intensity function and denotes as λ٭(t), which represents the infinitesimal rate at which events are expected to occur around a particular time &lt;em&gt;t.&lt;/em&gt; It is conditional on the prior history &lt;em&gt;H(t) *of the point process prior to time *t&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B3-3.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The behaviour of a simple temporal point process &lt;strong&gt;&lt;em&gt;N(t)&lt;/em&gt;&lt;/strong&gt; is typically modeled by specifying its &lt;em&gt;conditional intensity&lt;/em&gt;, λ٭(t).&lt;/p&gt;

&lt;p&gt;We introduced terms like ‘self-exciting’ and ‘self-regulating’ which can be easily understood using the conditional intensity function. If a recent arrival in history *H(t) *causes the conditional intensity function to increase then the process is said to be self-exciting. In general, λ٭(t) depends not only on *t *but also on the times T&lt;strong&gt;ᵢ&lt;/strong&gt; of preceding events i.e., is H(t). 
When N is &lt;strong&gt;Poisson point process&lt;/strong&gt;, the conditional intensity function λ٭(t) depends only on information about the current time, but not on history H(u). Poisson point process is neither self-exciting nor self-regulating.
 λ٭(t) is just function of over time for Poisson point process, stationary Poisson process has constant conditional rate: λ٭(t) = α, for all *t. *λ٭(t) = α implies that the probability of occurrence of an event is constant at any point of time regardless of how frequently such events have occurred previously.&lt;/p&gt;

&lt;h2 id=&quot;hawkes-process&quot;&gt;Hawkes process&lt;/h2&gt;

&lt;p&gt;The Hawkes process belongs to a family of self-exciting point processes named after its creator Alan G. Hawkes. Self-exciting point process models are used model events that are temporally clustered. Events like “earthquakes” and “panic selling of stocks” are often temporally clustered, i.e., the arrival of an event increases the likelihood of observing such events in the near future. Let’s define the Hawkes conditional intensity function — 
&lt;strong&gt;Definition&lt;/strong&gt; {t1, t2, . . . , tk} to denote the observed sequence of past arrival times of the point process up to time t, the Hawkes conditional intensity is&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B3-4.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The constant λ is called background intensity, μ(·) is called excitation function. if μ(·) equals zero then this self-exciting point process reduces to simple stationary poisson process. A common choice for excitation function, μ(·) is exponential decay.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B3-5.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Parameters α and β are the constants. α, β can be interpreted as that each arrival in the system instantaneously increases the arrival intensity by α, then over time, this arrival’s influence decays at rate β.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B3-6.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The modified Hawkes conditional intensity looks like this. Another frequently used excitation function in power law function.&lt;/p&gt;

&lt;p&gt;α and β are the parameters of λ٭(t) and let θ represent parameters. The parameter vector θ for a point process is estimated by maximizing the log-likelihood function. We can also use parametric functions to approximate conditional intensity function, we will discuss more about that in the next blog in this blog series. 
There is an obvious extension to the self exciting point process which is mutually-exciting point process. These are essentially set of one-dimensional point processes which excite themselves and each other. This set of point process are called &lt;strong&gt;multi-variate or mutually exciting point processes&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;If for each i = 1, . . . , m then each counting process N&lt;strong&gt;ᵢ&lt;/strong&gt;(t) has conditional intensity of the form:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B3-7.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;simulations&quot;&gt;Simulations&lt;/h2&gt;

&lt;p&gt;Let’s simulate a simple Hawkes point process: λ: 0.1, α:0.1, β:0.1 and try to understand conditional intensity function.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B3-8.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can clearly observe the excitation and decay in the above graph. Now let’s increase the background intensity λ to 0.5.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B3-9.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We clearly see the number of events increased as the background intensity increased and hovered above 0.5. We will now try to understand the impact of α, β. Let’s increase α to 0.5.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B3-10.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can clearly see that the number of events increased, this is because each occurrence of event increases the arrival intensity of next event by α. So λ٭(t) increase became higher, also one interesting observation is that λ٭(t) varied from 0.1 to 0.6.&lt;/p&gt;

&lt;p&gt;Now let’s increase the β to 0.5. Remember β controls the influence of decay rate of an event on its successive event.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B3-11.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Compare this plot with Fig 1, the decay is in Fig 1 is very less than the decay in Fig 4. 
The core concepts of Hawkes point process are demonstrated in the above examples.&lt;/p&gt;

&lt;p&gt;Point processes have extensive applications in various fields. We can model streams of discrete event/events in continuous time. We can also use function approximations where the conditional intensity functions of multiple event type can be approximated by novel neural architectures like LSTM. In the next blog in this series, we will discuss neural hawkes process.&lt;/p&gt;

&lt;p&gt;Thanks for your time😇&lt;/p&gt;</content><author><name>kowshik</name></author><category term="featured" /><summary type="html">Understanding Point Processes</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/kowshikchills.github.io/assets/images/B3-2.jpg" /></entry><entry><title type="html">The Cox Proportional-Hazards Model</title><link href="http://localhost:4000/kowshikchills.github.io/The-Cox-Proportional-Hazards-Model/" rel="alternate" type="text/html" title="The Cox Proportional-Hazards Model" /><published>2020-05-20T00:00:00+05:30</published><updated>2020-05-20T00:00:00+05:30</updated><id>http://localhost:4000/kowshikchills.github.io/The%20Cox%20Proportional-Hazards%20Model</id><content type="html" xml:base="http://localhost:4000/kowshikchills.github.io/The-Cox-Proportional-Hazards-Model/">&lt;h3 id=&quot;a-modelling-technique-to-estimate-the-survival-rates&quot;&gt;A Modelling Technique to Estimate the Survival Rates&lt;/h3&gt;

&lt;p&gt;Modelling time has been a topic of interest for scientists, sociologists, and even epidemiologists. A maintenance engineer wants to predict the time it takes for the next failure of a particular component in a vehicle engine occurs so that he can schedule preventive maintenance. It is of epidemiologist’s interest to predict when the next outbreak will occur, so he can plan for medical interventions. Business analyst want to understand the time it takes for an high values customer to churn so that he/she can take preventions measures.&lt;/p&gt;

&lt;p&gt;In our earlier blogs on point process model, we explored statistical techniques that estimate the likeliness of a certain event occurrence in the backdrop of the time dimension. In this new statistical techniques, we will keep the event in backdrop and model time. Survival models are statistical techniques used to estimate the length of time taken for an event to occur. We call event occurrence as &lt;em&gt;failure&lt;/em&gt; and &lt;em&gt;survival time&lt;/em&gt; is the time taken for such failure.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Cox proportional-hazards model&lt;/em&gt; is developed by Cox and published in his work[1] in 1972. It is the most commonly used regression model for survival data. The most interesting aspect of this survival modeling is it ability to examine the relationship between survival time and predictors. For example, if we are examining the survival of patients then the predictors can be age, blood pressure, gender, smoking habits, etc. These predictors are usually termed as covariates.&lt;/p&gt;

&lt;h2 id=&quot;hazard-function--λt-&quot;&gt;*Hazard Function ( *λ(t) )&lt;/h2&gt;

&lt;p&gt;The &lt;em&gt;hazard function *λ(t) is defined as the event rate at time *t&lt;/em&gt;. Suppose that an item has survived for a time t, then λ(t) is the probability that it will not survive for an additional time &lt;em&gt;dt. H&lt;/em&gt;azard function λ(t) gives the instantaneous risk of demise at time t, conditional on survival to that time and covariates.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B6-3.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Z is a vector of covariates&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;em&gt;λo(t)&lt;/em&gt;&lt;/strong&gt; is called the baseline hazard function&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;em&gt;Baseline hazard function&lt;/em&gt; describes how the risk of event per time unit changes over time. It is underlying hazard with all covariates Z1, …, Zp equal to 0.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B6-4.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;parameters-estimation&quot;&gt;Parameters Estimation&lt;/h2&gt;

&lt;p&gt;Cox proposed a partial likelihood for β without involving baseline hazard function &lt;strong&gt;&lt;em&gt;λo(t)&lt;/em&gt;&lt;/strong&gt;. The parameters of the Cox model can still be estimated by the method of partial likelihood without specifying the baseline hazard. The likelihood of the event to be observed occurring for subject j at time Xj can be written as&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B6-5.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Lⱼ(β) is probability that individual j fails give that there one failure from risk set. Partial Probability &lt;strong&gt;L&lt;/strong&gt;(β) = &lt;strong&gt;∏(L&lt;/strong&gt;ⱼ(β)).&lt;/p&gt;

&lt;p&gt;R(Xj) is called risk set, it denote the set of individuals who are “at risk” for failure at time t &lt;em&gt;[3]&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;This partial likelihood function can be maximised over &lt;em&gt;β&lt;/em&gt; to produce maximum partial likelihood estimates of the model parameters[2]. For convenience we apply the log to the partial likelihood function: 
&lt;strong&gt;log-partial likelihood( 𝓁(β))&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B6-6.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We differentiate log-partial likelihood( 𝓁(β)) and equate it to zero for calculating the β. The partial likelihood can be maximised using the &lt;a href=&quot;https://en.wikipedia.org/wiki/Newton%27s_method&quot;&gt;Newton-Raphson&lt;/a&gt; algorithm[2].&lt;/p&gt;

&lt;h2 id=&quot;python-implementation&quot;&gt;Python Implementation&lt;/h2&gt;

&lt;p&gt;Let’s jump into the final and most interesting section: implementation of CoxPH model in python with the help of lifelines package. An example dataset we will use is the Rossi recidivism dataset.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;**from** **lifelines** **import** CoxPHFitter
**from** **lifelines.datasets** **import** load_rossi
rossi_dataset = load_rossi()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B6-7.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;arrest column is the event occurred,&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The other columns represent predicates or covariates&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Week is the time scale&lt;/p&gt;

    &lt;p&gt;cph = CoxPHFitter()
cph.fit(rossi_dataset, duration_col=’week’, event_col=’arrest’)
cph.print_summary()&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B6-8.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B6-9.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;cph.plot() outputs this pictorial representation of coefficient for each predictor. The summary statistics above indicates the significance of the covariates in predicting the re-arrest risk. Age doesn’t play any significant role in predicting the re-arrest, whereas marriage variable plays significant role in predicting time for re-arrest.&lt;/p&gt;

&lt;p&gt;Lets look at a survival curve for one candidate with particular features(predicates/ covariates) using cph.predict_survival_function(df_vector).plot(). &lt;strong&gt;Survival rates (S(t))&lt;/strong&gt; simply gives us the probability that event will not occur beyond time t.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B6-10.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;we can also plot what the survival curves for single covariate i.e we keep all other covariates unchanged. This is useful to understand the impact of a covariate. we use[&lt;strong&gt;plot_covariate_groups()](https://lifelines.readthedocs.io/en/latest/fitters/regression/CoxPHFitter.html#lifelines.fitters.coxph_fitter.CoxPHFitter.plot_covariate_groups)&lt;/strong&gt; method and give it the covariate of interest, and the values to display[4].&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/B6-11.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can clearly see that the survival rates of married prisoner is higher than that of unmarried as married tends less to do crimes again as he got family to take care. We can simply deduce such similar and valuable insights from the above survival curves.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;We introduced the most famous survival model: Cox model; in this blog and understood its mathematical implementation. We also saw through its python implementation that the model has kept its promise of interpretability. There are more and robust model to discuss in survival model. We will discuss more examples and other famous survival models in the next blog in this series.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thanks for your time😀&lt;/em&gt;&lt;/p&gt;</content><author><name>kowshik</name></author><category term="CoxProcess" /><summary type="html">A Modelling Technique to Estimate the Survival Rates</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/kowshikchills.github.io/assets/images/B6-2.jpg" /></entry><entry><title type="html">Support Vector Machines</title><link href="http://localhost:4000/kowshikchills.github.io/Support-Vector-Machines/" rel="alternate" type="text/html" title="Support Vector Machines" /><published>2020-05-16T00:00:00+05:30</published><updated>2020-05-16T00:00:00+05:30</updated><id>http://localhost:4000/kowshikchills.github.io/Support%20Vector%20Machines</id><content type="html" xml:base="http://localhost:4000/kowshikchills.github.io/Support-Vector-Machines/">&lt;p&gt;Given a problem statement to categorize a set of data into classes, we can resort to algorithms like logistic regression, decision trees, boosting techniques etc., There is one more interesting and intuitive concept that helps in classification which is support vector machines. To understand SVM we must have clear idea on hyperplane, margin, kernel. Here is my attempt to help you understand these terms :)&lt;/p&gt;

&lt;h3 id=&quot;hyperplane&quot;&gt;Hyperplane:&lt;/h3&gt;

&lt;p&gt;Assume you have a 2D space with some data points as shown, and a line (ax+ by +c=0) is able to group this space into two parts.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/ML7-2.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Similarly for data in 3D space, a 2D plane can group the data into parts and for higher dimensions this works as well but it is hard to visualize. This flat line(1D)/plane(2D)/sub space of dimension p-1 which categorizes the given data space of p dimensions(features) is called a &lt;strong&gt;hyperplane&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Suppose we are solving a classification problem and the data is spread as shown in fig-1. If the algorithm is able to find the hyperplane then we are sorted because it classified all the points correctly. Let us understand this approach in depth.&lt;/p&gt;

&lt;p&gt;For a p-dimension hyperplane, if a point X = (X1,X2,…,Xp) in p-dimensional space (i.e. a vector of length p) satisﬁes eq-1, then X lies on the hyperplane.
&lt;img src=&quot;/kowshikchills.github.io/assets/images/ML7-3.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If X does not satisfy the equation and instead if β0 + β1X1 + β2X2 + …+ βpXp &amp;gt; 0 then it lies on one side of the hyper plane and if β0 + β1X1 + β2X2 + …+ βpXp &amp;lt;0 then X lies on the other side of the hyper plane. With this equation we can divide the whole data space into 2 parts and can easily classify the new data point based on the above conditions. For example, a hyperplane of 1+2X1+3X2=0 will look like:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/ML7-4.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There is a data set which looks as shown below. We can bring up many hyper planes, let us understand the A,B,C hyper planes as shown below. C is able to correctly classify and B had few errors and A has many errors in classifying the data. But how does the algorithm pick if A,B,C as best hyperplane to classify data?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/ML7-5.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;margin&quot;&gt;Margin:&lt;/h3&gt;

&lt;p&gt;Let us take the same data space with only one hyperplane, the lines to the closest data points from the hyperplane are represented as M1 and M2 ( these should be perpendicular to the hyperplane selected) and the distance from the hyperplane to either M1 or M2 is called &lt;strong&gt;marign&lt;/strong&gt; and these nearest points where arrows are put are called &lt;strong&gt;support vectors&lt;/strong&gt;. Support vectors lie along the margins and indicates the length of margin thus supporting the hyperplane because if these points are moved either near or away from hyperplane, then margin is decreased or increased respectively. If we observe closely, the hyperplane and margin is dependent only on support vectors and not on any other data points.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/ML7-6.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For different hyper planes we have different lengths of margins, and the hyperplane which has the maximum margin is called the &lt;strong&gt;Maximal margin hyperplane&lt;/strong&gt; or the &lt;strong&gt;optimal separating hyperplane&lt;/strong&gt;. If we are categorizing a data point into a class based on the position it lies with respect to the maximal margin hyperplane (left or right of hyperplane) then it is called &lt;strong&gt;maximal margin classifier&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;For this kind of data set as shown in the figure above, the hyperplane is able to correctly categorize the points but its not the same always, many real time data sets might have hyper planes having misclassified data points as well. This an important point to make while learning SVM. This helps in adding robustness to the model when using on test data.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/ML7-7.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For example in fig 2, point 1,8 are to the wrong side of their respective margins but to the correct side of hyperplane. Points 11 and 12 are located on the wrong side of hyperplane and margin as well.&lt;/p&gt;

&lt;p&gt;What happens with different lengths of M? If margin is large i.e., if the support vectors are far from hyperplane, then it is obvious that the data points are well distanced enough and a new point coming in can be easily classified with very less error. If M is small, then the support vectors are really close to hyperplane and any new point coming in has a very high probability of misclassification. So optimization would be to maximize M(width of margin). Mathematically,&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/ML7-8.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;here, yi is the classification variable where it takes values of 1 and -1 only for the two classes present, C is a non negative tuning parameter. If β0,β1,…,βp are the coefficients of the maximal margin hyperplane, then a new test data point x is classified based on the &lt;strong&gt;sign&lt;/strong&gt; of f(x&lt;em&gt;) = β0 + β1x1&lt;/em&gt; + β2x2* + …+ βpxp*. epsilon-1,…,n are slack variables that allow individual observations to be on the wrong side of the margin or the hyperplane. If epsilon-i = 0 then the ith observation is on the correct side of the margin. If epsilon-i &amp;gt; 0 then the ith observation is on the wrong side of the margin, and we say that the ith observation has violated the margin. If epsilon-i &amp;gt; 1 then it is on the wrong side of the hyperplane as explained in fig-2.&lt;/p&gt;

&lt;p&gt;The role of the tuning parameter C — It bounds the sum of the epsilon-i’s, so it determines the number and severity of the violations to the margin (and to the hyperplane) that we will tolerate. C is the assignment of amount that margin can be violated by n observations.&lt;/p&gt;

&lt;p&gt;— If C = 0 then there is no budget for violations to the margin, and it must be the case that epsilon-1 = …= epsilon-n = 0, in which simply amounts to the maximum margin hyperplane (in the equation above).&lt;/p&gt;

&lt;p&gt;— C is picked using cross-validation. If C is small, the margins are closer and the model has high fit on train data with low bias but high variance. If C is larger then margins have more width and a new data point has a high probability of violating the hyperplane separation, thus leading to more bias but lower variance. Tuning all these parameters will result in a good classifier.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/ML7-9.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For a random data with blue and red points as different classes, as the value of C increases, the model is clearly intolerant of misclassified data and tries to classify all of them correctly by adjusting the hyperplane.&lt;/p&gt;

&lt;h3 id=&quot;kernels&quot;&gt;Kernels:&lt;/h3&gt;

&lt;p&gt;A question that might already have arised looking at fig-1 is that what if the data points are spread randomly in the space? what if there is no linear boundary separating the classes- then how does SVM work? — The trick is with kernels.&lt;/p&gt;

&lt;p&gt;To deal with this problem of non- linear boundary between classes, we can transform the feature space to a large size using polynomial function of predictors by making them squared or cubed or to any higher degree. But doing so would lead to computational inability because of huge predictor space. Here is where kernels have an upper hand in ruling out computational roadblocks. So let us try to understand it.&lt;/p&gt;

&lt;p&gt;Looking at eq-2 above, it is comprised of a basic mathematical computation — inner product of the observations. Inner product of any two observations xi,xi’ is&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/ML7-10.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For a linear boundary, this is one representation, there will be different representations for non-linear boundaries which we will see soon. So when ever we refer to a function (like inner product here) we can write a generalized form K (xi,xi’). K is any function and it is referred as Kernel here which measures the similarity of two observations.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/ML7-11.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So, the support vector classifier with any kernel K is represented as&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/ML7-12.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;where there are n parameters αi,i=1,…,n, one per training observation. However, it turns out that αi is nonzero only for the support vectors in the solution — that is, if a training observation is not a support vector, then its αi equals zero. S is the collection of indices of these support points.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;polynomial&lt;/strong&gt; kernel can be defined as above with degree &lt;strong&gt;d&lt;/strong&gt;. Instead of using linear kernel, polynomial kernel provides a much better decision boundary for classifying the data. So, a kernel essentially amounts to ﬁtting a support vector classiﬁer in a higher-dimensional space involving polynomials of degree d, rather than in the original feature space.&lt;/p&gt;

&lt;p&gt;Another popular kernel used is &lt;strong&gt;radial&lt;/strong&gt; kernel which has gamma as a positive constant that has a great impact on the model. If a given test observation x* =( x1&lt;em&gt;, …xp&lt;/em&gt;) is far from a training observation xi in terms of Euclidean distance is large but the equation in radial kernel discounts it using gamma and the distance is greatly reduced, so xi will play virtually no role in f(x&lt;em&gt;) — eq-3 above. Recall that the predicted class label for the test observation x&lt;/em&gt; is based on the sign of f(x&lt;em&gt;). In other words, training observations that are far from x&lt;/em&gt; will play essentially no role in the predicted class label for x*. This means that the &lt;strong&gt;radial&lt;/strong&gt; kernel has very &lt;strong&gt;local&lt;/strong&gt; behavior, in the sense that only nearby training observations have an eﬀect on the class label of a test observation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/ML7-13.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Deploying kernels with with support vectors for non linear boundaries is called &lt;strong&gt;Support Vector Machine.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/ML7-14.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A little comparison between these kernels would do no harm :P &lt;strong&gt;Time&lt;/strong&gt; taken to train the model , ability to &lt;strong&gt;fit&lt;/strong&gt; data and risk of &lt;strong&gt;overfitting&lt;/strong&gt;: linear&amp;lt; polynomial &amp;lt; radial&lt;/p&gt;

&lt;h3 id=&quot;learning-in-python&quot;&gt;Learning in python&lt;/h3&gt;

&lt;p&gt;Like all other models, SVM is also included in sklearn package. We have various parameters in sklearn.svm where SVC function is for classification and linearSVC for regression. As we talked about classification here, lets check out the important parameters in SVC function that might greatly effect the results.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;C&lt;/strong&gt;: This is a regularization parameter that we have in eq-2 as explained above. default C value is 1&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;kernel&lt;/strong&gt;: Values that go into this parameter are ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’, default =’rbf’ (rbf- radial basis function)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;degree&lt;/strong&gt;: If a polynomial kernel is in use, then we have to specify the degree here. For rest of the kernels this will not be used. default is 3&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;gamma&lt;/strong&gt;: this is another important parameter while using radial, polynomial or sigmoid kernels.&lt;/p&gt;

&lt;p&gt;Let us understand how this is implemented in python with a tutorial. I have taken breast cancer anlaysis data set from kaggle for classification. For data cleaning and EDA — please check &lt;a href=&quot;https://medium.com/all-about-ml/logistic-regression-8510e9bc477a&quot;&gt;this blog&lt;/a&gt; if you are interested.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from sklearn.svm import SVC
from sklearn.metrics import accuracy_score,f1_score

c_range = [0.01,0.1,1,10,100]

eval_metric = pd.DataFrame(columns = [‘C_parameter’,’Accuracy’])
eval_metric[‘C_parameter’] = c_range

j = 0

for i in c_range:
 svm_linear = SVC(kernel = ‘linear’, C = i, random_state = 0)
 svm_linear.fit(x_train,y_train)
 y_pred = svm_linear.predict(x_test)
 eval_metric.iloc[j,1] = metrics.accuracy_score(y_test,y_pred)
 j += 1

print(eval_metric)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/kowshikchills.github.io/assets/images/ML7-15.jpg&quot; alt=&quot;walking&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I have taken a linear kernel with different values of C and fit the train data into the model. We observe that for C being 1 or 10 or 100, the model has not improved much and got a decent accuracy score. We can similarly change different parameters like kernel to rbf or poly etc., and pick the one that fits the requirement.&lt;/p&gt;

&lt;p&gt;Hope this helps! thank you!!&lt;/p&gt;

&lt;p&gt;References: An Introduction to Statistical Learning: With Applications in R&lt;/p&gt;</content><author><name>dharani</name></author><category term="featured" /><summary type="html">Given a problem statement to categorize a set of data into classes, we can resort to algorithms like logistic regression, decision trees, boosting techniques etc., There is one more interesting and intuitive concept that helps in classification which is support vector machines. To understand SVM we must have clear idea on hyperplane, margin, kernel. Here is my attempt to help you understand these terms :)</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/kowshikchills.github.io/assets/images/ML7-1.jpg" /></entry></feed>